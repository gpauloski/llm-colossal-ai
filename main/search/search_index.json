{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LLM Training Scripts","text":"<p>Tools and training scripts I have developed for building large language models in PyTorch.</p> <p>This repository provides:</p> <ul> <li>data preprocessing scripts,</li> <li>training scripts, and</li> <li>training guides.</li> </ul> <p>This repository is the successor to my old training tools BERT-PyTorch as the old code had a lot of technical debt and was not well tested. Compared to the old repository, this codebase aims to have better code health and maintainability thanks to tests, type checking, linters, documentation, etc.</p>"},{"location":"#install","title":"Install","text":"<p>See the Installation Guide.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>See the available Guides.</p>"},{"location":"known-issues/","title":"Known Issues","text":"<p>There are no known issues at the time. If you encounter a problem, consider opening an issue.</p>"},{"location":"api/","title":"llm","text":"<code>llm/__init__.py</code> <p>LLM package.</p> <p>Large language model training tools. Preprocessing scripts are provided in <code>llm.preprocess</code>, and training scripts in <code>llm.trainers</code>.</p>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>llm<ul> <li>llm.checkpoint</li> <li>llm.config</li> <li>llm.datasets<ul> <li>llm.datasets.bert</li> <li>llm.datasets.roberta</li> <li>llm.datasets.sharded</li> </ul> </li> <li>llm.engine<ul> <li>llm.engine.accumulation</li> <li>llm.engine.amp</li> <li>llm.engine.base</li> <li>llm.engine.initialize</li> </ul> </li> <li>llm.environment</li> <li>llm.initialize</li> <li>llm.loss</li> <li>llm.models<ul> <li>llm.models.bert</li> </ul> </li> <li>llm.optimizers</li> <li>llm.preprocess<ul> <li>llm.preprocess.download</li> <li>llm.preprocess.format</li> <li>llm.preprocess.roberta</li> <li>llm.preprocess.shard</li> <li>llm.preprocess.tokenize</li> <li>llm.preprocess.utils</li> <li>llm.preprocess.vocab</li> </ul> </li> <li>llm.schedulers</li> <li>llm.timer</li> <li>llm.trainers<ul> <li>llm.trainers.bert<ul> <li>llm.trainers.bert.main</li> <li>llm.trainers.bert.utils</li> </ul> </li> </ul> </li> <li>llm.utils</li> </ul> </li> </ul>"},{"location":"api/checkpoint/","title":"llm.checkpoint","text":"<code>llm/checkpoint.py</code>"},{"location":"api/checkpoint/#llm.checkpoint.Checkpoint","title":"Checkpoint","text":"<p>         Bases: <code>NamedTuple</code></p> <p>Data loaded from a checkpoint.</p>"},{"location":"api/checkpoint/#llm.checkpoint.Checkpoint.filepath","title":"filepath  <code>class-attribute</code>","text":"<pre><code>filepath: str\n</code></pre> <p>Filepath of checkpoint.</p>"},{"location":"api/checkpoint/#llm.checkpoint.Checkpoint.global_step","title":"global_step  <code>class-attribute</code>","text":"<pre><code>global_step: int\n</code></pre> <p>Global step of checkpoint.</p>"},{"location":"api/checkpoint/#llm.checkpoint.Checkpoint.model_state_dict","title":"model_state_dict  <code>class-attribute</code>","text":"<pre><code>model_state_dict: dict[Any, Any]\n</code></pre> <p>Model state dictionary.</p>"},{"location":"api/checkpoint/#llm.checkpoint.Checkpoint.optimizer_state_dict","title":"optimizer_state_dict  <code>class-attribute</code>","text":"<pre><code>optimizer_state_dict: dict[Any, Any] | None\n</code></pre> <p>Optimizer state dictionary.</p>"},{"location":"api/checkpoint/#llm.checkpoint.Checkpoint.scheduler_state_dict","title":"scheduler_state_dict  <code>class-attribute</code>","text":"<pre><code>scheduler_state_dict: dict[Any, Any] | None\n</code></pre> <p>Scheduler state dictionary.</p>"},{"location":"api/checkpoint/#llm.checkpoint.Checkpoint.kwargs","title":"kwargs  <code>class-attribute</code>","text":"<pre><code>kwargs: dict[str, Any]\n</code></pre> <p>Additional keyword arguments stored in the checkpoint.</p>"},{"location":"api/checkpoint/#llm.checkpoint.load_checkpoint","title":"load_checkpoint()","text":"<pre><code>load_checkpoint(\ncheckpoint_dir: str | pathlib.Path,\nglobal_step: int | None = None,\nmap_location: Any = None,\n) -&gt; Checkpoint | None\n</code></pre> <p>Load checkpoint from directory.</p> <p>Parameters:</p> <ul> <li> checkpoint_dir             (<code>str | pathlib.Path</code>)         \u2013 <p>Directory containing checkpoint files.</p> </li> <li> global_step             (<code>int | None</code>)         \u2013 <p>Global step checkpoint to load. If <code>None</code>, loads the latest checkpoint.</p> </li> <li> map_location             (<code>Any</code>)         \u2013 <p>Optional map_location to pass to <code>torch.load()</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Checkpoint | None</code>         \u2013 <p>Checkpoint or <code>None</code> if no checkpoint was found.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>OSError</code>           \u2013         <p>If <code>checkpoint_dir</code> does not exist.</p> </li> <li> <code>OSError</code>           \u2013         <p>If <code>global_step</code> is specified but the file does not exist.</p> </li> </ul> Source code in <code>llm/checkpoint.py</code> <pre><code>def load_checkpoint(\ncheckpoint_dir: str | pathlib.Path,\nglobal_step: int | None = None,\nmap_location: Any = None,\n) -&gt; Checkpoint | None:\n\"\"\"Load checkpoint from directory.\n    Args:\n        checkpoint_dir: Directory containing checkpoint files.\n        global_step: Global step checkpoint to load. If `None`,\n            loads the latest checkpoint.\n        map_location: Optional map_location to pass to\n            [`torch.load()`][torch.load].\n    Returns:\n        Checkpoint or `None` if no checkpoint was found.\n    Raises:\n        OSError: If `checkpoint_dir` does not exist.\n        OSError: If `global_step` is specified but the file does not exist.\n    \"\"\"\ndir_path = pathlib.Path(checkpoint_dir)\nif not dir_path.is_dir():\nraise OSError(f'Checkpoint directory {checkpoint_dir} does not exist.')\nif global_step is None:\ncheckpoints = {\nmatch.group(1): str(p)\nfor p in dir_path.iterdir()\nif (match := CHECKPOINT_NAME_RE.fullmatch(p.name)) is not None\n}\nif len(checkpoints) == 0:\nreturn None\nsteps = [int(key) for key in checkpoints]\nglobal_step = max(steps)\nassert global_step is not None\ncheckpoint_path = dir_path / f'global_step_{global_step}.pt'\nif not checkpoint_path.is_file():\nraise OSError(f'Checkpoint named {checkpoint_path} does not exist.')\nstate_dict = torch.load(checkpoint_path, map_location=map_location)\nmodel_state_dict = state_dict.pop('model')\noptimizer_state_dict = state_dict.pop('optimizer', None)\nscheduler_state_dict = state_dict.pop('scheduler', None)\nreturn Checkpoint(\nfilepath=str(checkpoint_path),\nglobal_step=global_step,\nmodel_state_dict=model_state_dict,\noptimizer_state_dict=optimizer_state_dict,\nscheduler_state_dict=scheduler_state_dict,\nkwargs=state_dict,\n)\n</code></pre>"},{"location":"api/checkpoint/#llm.checkpoint.save_checkpoint","title":"save_checkpoint()","text":"<pre><code>save_checkpoint(\ncheckpoint_dir: str | pathlib.Path,\nglobal_step: int,\nmodel: torch.nn.Module,\noptimizer: torch.optim.Optimizer | None = None,\nscheduler: torch.optim.lr_scheduler._LRScheduler\n| None = None,\n**kwargs: Any\n) -&gt; None\n</code></pre> <p>Save checkpoint to directory.</p> <p>Saves the checkpoint as <code>{checkpoint_dir}/global_step_{global_step}.py</code>.</p> <p>Parameters:</p> <ul> <li> checkpoint_dir             (<code>str | pathlib.Path</code>)         \u2013 <p>Directory to save checkpoint to.</p> </li> <li> global_step             (<code>int</code>)         \u2013 <p>Training step used as the key for checkpoints.</p> </li> <li> model             (<code>torch.nn.Module</code>)         \u2013 <p>Model to save state_dict of.</p> </li> <li> optimizer             (<code>torch.optim.Optimizer | None</code>)         \u2013 <p>Optional optimizer to save state_dict of.</p> </li> <li> scheduler             (<code>torch.optim.lr_scheduler._LRScheduler | None</code>)         \u2013 <p>Optional scheduler to save state_dict of.</p> </li> <li> kwargs             (<code>Any</code>)         \u2013 <p>Additional key-value pairs to add to the checkpoint.</p> </li> </ul> Source code in <code>llm/checkpoint.py</code> <pre><code>def save_checkpoint(\ncheckpoint_dir: str | pathlib.Path,\nglobal_step: int,\nmodel: torch.nn.Module,\noptimizer: torch.optim.Optimizer | None = None,\nscheduler: torch.optim.lr_scheduler._LRScheduler | None = None,\n**kwargs: Any,\n) -&gt; None:\n\"\"\"Save checkpoint to directory.\n    Saves the checkpoint as `{checkpoint_dir}/global_step_{global_step}.py`.\n    Args:\n        checkpoint_dir: Directory to save checkpoint to.\n        global_step: Training step used as the key for checkpoints.\n        model: Model to save state_dict of.\n        optimizer: Optional optimizer to save state_dict of.\n        scheduler: Optional scheduler to save state_dict of.\n        kwargs: Additional key-value pairs to add to the checkpoint.\n    \"\"\"\nstate_dict = {'model': model.state_dict(), **kwargs}\nif optimizer is not None:\nstate_dict['optimizer'] = optimizer.state_dict()\nif scheduler is not None:\nstate_dict['scheduler'] = scheduler.state_dict()\ndir_path = pathlib.Path(checkpoint_dir)\ndir_path.mkdir(parents=True, exist_ok=True)\ncheckpoint_path = dir_path / f'global_step_{global_step}.pt'\ntorch.save(state_dict, checkpoint_path)\n</code></pre>"},{"location":"api/cli/","title":"CLI Reference","text":""},{"location":"api/cli/#cli-reference","title":"CLI Reference","text":"<p>This page provides documentation for our command line tools.</p> <p>Warning</p> <p>The usage examples show the executable module the CLI belongs to. To run the CLI, you must execute the module using the Python interpreter. E.g., <pre><code>$ python -m llm.preprocess.download --help\n</code></pre></p> <p>Note</p> <p>This list is not exhaustive. In particular, the training scripts provided in the <code>llm.trainers</code> module are not listed here.</p>"},{"location":"api/cli/#llmpreprocessdownload","title":"llm.preprocess.download","text":"<p>Pretraining text downloader.</p> <p>Usage:</p> <pre><code>llm.preprocess.download [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--dataset</code> choice (<code>wikipedia</code> | <code>bookscorpus</code>) Dataset to download. _required <code>--output</code> text Output directory. _required <code>--log-level</code> choice (<code>DEBUG</code> | <code>INFO</code> | <code>WARNING</code> | <code>ERROR</code>) Minimum logging level. <code>INFO</code> <code>--rich</code> / <code>--no-rich</code> boolean Use rich output formatting. <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"api/cli/#llmpreprocessroberta","title":"llm.preprocess.roberta","text":"<p>RoBERTa pre-training dataset encoder.</p> <p>Usage:</p> <pre><code>llm.preprocess.roberta [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--input</code> text Glob of input shards to encode. _required <code>--output-dir</code> text Output directory for encoded shards. _required <code>--vocab</code> text Vocabulary file. _required <code>--tokenizer</code> choice (<code>bpe</code> | <code>wordpiece</code>) Tokenizer type. _required <code>--cased</code> / <code>--uncased</code> boolean Vocab/tokenizer is case-sensitive. <code>False</code> <code>--max-seq-len</code> integer Maximum sequence length. <code>512</code> <code>--short-seq-prob</code> float Probablity to create shorter sequences. <code>0.1</code> <code>-p</code>, <code>--processes</code> integer Number of processes for concurrent shard encoding. <code>4</code> <code>--log-level</code> choice (<code>DEBUG</code> | <code>INFO</code> | <code>WARNING</code> | <code>ERROR</code>) Minimum logging level. <code>INFO</code> <code>--rich</code> / <code>--no-rich</code> boolean Use rich output formatting. <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"api/cli/#llmpreprocessshard","title":"llm.preprocess.shard","text":"<p>Pre-training text sharder.</p> <p>Usage:</p> <pre><code>llm.preprocess.shard [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--input</code> text Glob of input shards to encode. _required <code>--output</code> text Output directory for encoded shards. _required <code>--size</code> text Max data size of each shard. _required <code>--format</code> text Shard name format where {index} is replaced by shard index. <code>shard-{index}.txt</code> <code>--shuffle</code> / <code>--no-shuffle</code> boolean Shuffle documents before sharding. <code>False</code> <code>--log-level</code> choice (<code>DEBUG</code> | <code>INFO</code> | <code>WARNING</code> | <code>ERROR</code>) Minimum logging level. <code>INFO</code> <code>--rich</code> / <code>--no-rich</code> boolean Use rich output formatting. <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"api/cli/#llmpreprocessvocab","title":"llm.preprocess.vocab","text":"<p>Pre-training vocabulary builder.</p> <p>Arguments default to the standard uncased BERT with wordpiece method.</p> <p>Usage:</p> <pre><code>llm.preprocess.vocab [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--input</code> text Glob of input text files to build vocab from. _required <code>--output</code> text Output filepath for vocabulary. _required <code>--size</code> integer Size of vocabulary. <code>30522</code> <code>--tokenizer</code> choice (<code>bpe</code> | <code>wordpiece</code>) Tokenizer type. <code>wordpiece</code> <code>--cased</code> / <code>--uncased</code> boolean Vocab/tokenizer is case-sensitive. <code>False</code> <code>-s</code>, <code>--special-token</code> text Special tokens to prepend to vocab. <code>['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']</code> <code>--log-level</code> choice (<code>DEBUG</code> | <code>INFO</code> | <code>WARNING</code> | <code>ERROR</code>) Minimum logging level. <code>INFO</code> <code>--rich</code> / <code>--no-rich</code> boolean Use rich output formatting. <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"api/config/","title":"llm.config","text":"<code>llm/config.py</code>"},{"location":"api/config/#llm.config.HParamT","title":"HParamT  <code>module-attribute</code>","text":"<pre><code>HParamT = Union[bool, float, int, str, None]\n</code></pre> <p>Supported Hyperparameter types (i.e., JSON types).</p>"},{"location":"api/config/#llm.config.Config","title":"Config","text":"<pre><code>Config(\nmapping: Mapping[str, Any]\n| Iterable[tuple[str, Any]]\n| None = None,\n/,\n**kwargs: Any,\n)\n</code></pre> <p>         Bases: <code>dict[str, Any]</code></p> <p>Dict-like configuration class with attribute access.</p> Example <pre><code>&gt;&gt;&gt; from llm.config import Config\n&gt;&gt;&gt; config = Config({'a': 1, 'b': 2})\n&gt;&gt;&gt; config.a\n1\n&gt;&gt;&gt; config = Config(a=1, b={'c': 2})\n&gt;&gt;&gt; config.b.c\n2\n</code></pre> <p>Parameters:</p> <ul> <li> mapping             (<code>Mapping[str, Any] | Iterable[tuple[str, Any]] | None</code>)         \u2013 <p>Initial mapping or iterable of tuples of key-value pairs.</p> </li> <li> kwargs             (<code>Any</code>)         \u2013 <p>Keywords arguments to add a key-value pairs to the config.</p> </li> </ul> Source code in <code>llm/config.py</code> <pre><code>def __init__(\nself,\nmapping: Mapping[str, Any] | Iterable[tuple[str, Any]] | None = None,\n/,\n**kwargs: Any,\n):\nif mapping is not None:\nif isinstance(mapping, Mapping):\nmapping = mapping.items()\nfor key, value in mapping:\nself.__setattr__(key, value)\nfor key, value in kwargs.items():\nself.__setattr__(key, value)\n</code></pre>"},{"location":"api/config/#llm.config.flattened_config","title":"flattened_config()","text":"<pre><code>flattened_config(\nconfig: dict[str, Any] | Config | None = None\n) -&gt; dict[str, HParamT]\n</code></pre> <p>Convert a config to a flat JSONable dictionary.</p> Note <p>If <code>torch.distributed.is_initialized()</code>, the <code>world_size</code> will be added to the config.</p> <p>Parameters:</p> <ul> <li> config             (<code>dict[str, Any] | Config | None</code>)         \u2013 <p>Optional starting config.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, HParamT]</code>         \u2013 <p>Flat dictionary containing only <code>bool</code>, <code>float</code>, <code>int</code>, <code>str</code>, or</p> </li> <li> <code>dict[str, HParamT]</code>         \u2013 <p><code>None</code> values.</p> </li> </ul> Source code in <code>llm/config.py</code> <pre><code>def flattened_config(\nconfig: dict[str, Any] | Config | None = None,\n) -&gt; dict[str, HParamT]:\n\"\"\"Convert a config to a flat JSONable dictionary.\n    Note:\n        If [`torch.distributed.is_initialized()`][torch.distributed.is_initialized],\n        the `world_size` will be added to the config.\n    Args:\n        config: Optional starting config.\n    Returns:\n        Flat dictionary containing only `bool`, `float`, `int`, `str`, or\n        `None` values.\n    \"\"\"\nif config is None:\nconfig = {}\nif dist.is_initialized():\nconfig['world_size'] = dist.get_world_size()\nconfig = flatten_mapping(config)\nfor key in list(config.keys()):\nif not isinstance(config[key], (bool, float, int, str, type(None))):\ndel config[key]\nreturn config\n</code></pre>"},{"location":"api/config/#llm.config.flatten_mapping","title":"flatten_mapping()","text":"<pre><code>flatten_mapping(\nd: Mapping[str, Any],\nparent: str | None = None,\nsep: str = \"_\",\n) -&gt; dict[str, Any]\n</code></pre> <p>Flatten mapping into dict by joining nested keys via a separator.</p> Warning <p>This function does not check for key collisions. E.g., <pre><code>&gt;&gt;&gt; flatten_mapping({'a': {'b_c': 1}, 'a_b': {'c': 2}})\n{'a_b_c': 2}\n</code></pre></p> <p>Parameters:</p> <ul> <li> d             (<code>Mapping[str, Any]</code>)         \u2013 <p>Input mapping. All keys and nested keys must by strings.</p> </li> <li> parent             (<code>str | None</code>)         \u2013 <p>Parent key to prepend to top-level keys in <code>d</code>.</p> </li> <li> sep             (<code>str</code>)         \u2013 <p>Separator between keys.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>         \u2013 <p>Flattened dictionary.</p> </li> </ul> Source code in <code>llm/config.py</code> <pre><code>def flatten_mapping(\nd: Mapping[str, Any],\nparent: str | None = None,\nsep: str = '_',\n) -&gt; dict[str, Any]:\n\"\"\"Flatten mapping into dict by joining nested keys via a separator.\n    Warning:\n        This function does not check for key collisions. E.g.,\n        ```python\n        &gt;&gt;&gt; flatten_mapping({'a': {'b_c': 1}, 'a_b': {'c': 2}})\n        {'a_b_c': 2}\n        ```\n    Args:\n        d: Input mapping. All keys and nested keys must by strings.\n        parent: Parent key to prepend to top-level keys in `d`.\n        sep: Separator between keys.\n    Returns:\n        Flattened dictionary.\n    \"\"\"\n# https://stackoverflow.com/questions/6027558\nitems: list[tuple[str, Any]] = []\nfor key, value in d.items():\nnew_key = f'{parent}{sep}{key}' if parent is not None else key\nif isinstance(value, collections.abc.Mapping):\nitems.extend(flatten_mapping(value, new_key, sep).items())\nelse:\nitems.append((new_key, value))\nreturn dict(items)\n</code></pre>"},{"location":"api/config/#llm.config.load_config","title":"load_config()","text":"<pre><code>load_config(filepath: pathlib.Path | str) -&gt; Config\n</code></pre> <p>Load Python file as a <code>Config</code>.</p> Note <p>Attributes starting with <code>_</code>, modules, classes, functions, and builtins will not be loaded from the Python file.</p> <p>Parameters:</p> <ul> <li> filepath             (<code>pathlib.Path | str</code>)         \u2013 <p>Python file to load.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Config</code>         \u2013 <p>Configuration attributes loaded from the Python file.</p> </li> </ul> Source code in <code>llm/config.py</code> <pre><code>def load_config(filepath: pathlib.Path | str) -&gt; Config:\n\"\"\"Load Python file as a [`Config`][llm.config.Config].\n    Note:\n        Attributes starting with `_`, modules, classes, functions, and\n        builtins will not be loaded from the Python file.\n    Args:\n        filepath: Python file to load.\n    Returns:\n        Configuration attributes loaded from the Python file.\n    \"\"\"\nfilepath = pathlib.Path(filepath).absolute()\nif not filepath.exists():\nraise OSError(f'{filepath} does not exist.')\nelif filepath.suffix != '.py':\nraise ValueError(\nf'{filepath} is not a Python file. Only .py files are supported.',\n)\nloader = SourceFileLoader(fullname=filepath.stem, path=str(filepath))\nmodule = types.ModuleType(loader.name)\nloader.exec_module(module)\nattrs: dict[str, Any] = {}\nfor key, value in module.__dict__.items():\nif not (\nkey.startswith('_')\nor inspect.ismodule(value)\nor inspect.isclass(value)\nor inspect.isfunction(value)\nor inspect.isbuiltin(value)\n):\nattrs[key] = value\nreturn Config(**attrs)\n</code></pre>"},{"location":"api/environment/","title":"llm.environment","text":"<code>llm/environment.py</code> <p>Utilities for collecting information about the environment.</p> Tip <p>This module is executable so you can easily check what resources your scripts will see as available. This is useful if you need to debug what software versions are being used or what hardware is visible by PyTorch. <pre><code>python -m llm.environment\n</code></pre></p>"},{"location":"api/environment/#llm.environment.Environment","title":"Environment","text":"<p>         Bases: <code>NamedTuple</code></p> <p>Named tuple representing collected environment information.</p>"},{"location":"api/environment/#llm.environment.collect_pip_version","title":"collect_pip_version()","text":"<pre><code>collect_pip_version() -&gt; str\n</code></pre> <p>Collect the pip version.</p> Source code in <code>llm/environment.py</code> <pre><code>def collect_pip_version() -&gt; str:\n\"\"\"Collect the pip version.\"\"\"\noutput = subprocess.check_output(['pip', '--version']).decode('utf-8')\nreturn output.split(' ')[1]\n</code></pre>"},{"location":"api/environment/#llm.environment.collect_pip_packages","title":"collect_pip_packages()","text":"<pre><code>collect_pip_packages() -&gt; list[str]\n</code></pre> <p>Collect a list of relevant pip packages.</p> Source code in <code>llm/environment.py</code> <pre><code>def collect_pip_packages() -&gt; list[str]:\n\"\"\"Collect a list of relevant pip packages.\"\"\"\noutput = subprocess.check_output(['pip', 'freeze']).decode('utf-8')\npackages = output.split('\\n')\nnames = [\n'torch',\n'numpy',\n'mypy',\n'colossalai',\n'h5py',\n'tensorboard',\n'tokenizers',\n'transformers',\n]\npackages = [\np.strip() for p in packages if any(name in p for name in names)\n]\nreturn sorted(packages)\n</code></pre>"},{"location":"api/environment/#llm.environment.collect_environment","title":"collect_environment()","text":"<pre><code>collect_environment() -&gt; Environment\n</code></pre> <p>Collects information on the hardware and software environment.</p> Source code in <code>llm/environment.py</code> <pre><code>def collect_environment() -&gt; Environment:\n\"\"\"Collects information on the hardware and software environment.\"\"\"\nrun_lambda = collect_env.run\nbit_count = sys.maxsize.bit_length() + 1\nsys_version = sys.version.replace('\\n', ' ')\npip_version = collect_pip_version()\npip_packages = collect_pip_packages()\nversion_str = torch.__version__\ndebug_mode_str = torch.version.debug\npcores = psutil.cpu_count(logical=False)\nlcores = psutil.cpu_count(logical=True)\ncpu_info = f'{platform.processor()} ({pcores} cores / {lcores} logical)'\ntotal_ram = round(psutil.virtual_memory().available / 1e9, 2)\ncuda_available_str = torch.cuda.is_available()\ncuda_version_str = torch.version.cuda\nreturn Environment(\nos=collect_env.get_os(run_lambda),\npython_version=f'{sys_version} ({bit_count}-bit runtime)',\npython_platform=collect_env.get_python_platform(),\npip_version=pip_version,\npip_packages='\\n'.join(pip_packages),\ntorch_version=version_str,\ntorch_is_debug=debug_mode_str,\ncpu_info=cpu_info,\ntotal_ram_gb=total_ram,\ncuda_is_available=cuda_available_str,\ncuda_compiled_version=cuda_version_str,\ncuda_runtime_version=collect_env.get_running_cuda_version(run_lambda),\ncuda_module_loading=collect_env.get_cuda_module_loading_config(),\nnvidia_gpu_models=collect_env.get_gpu_info(run_lambda),\nnvidia_driver=collect_env.get_nvidia_driver_version(run_lambda),\ncudnn_version=collect_env.get_cudnn_version(run_lambda),\n)\n</code></pre>"},{"location":"api/environment/#llm.environment.log_environment","title":"log_environment()","text":"<pre><code>log_environment(\nlevel: int = logging.INFO,\nranks: Iterable[int] | None = (0),\n) -&gt; None\n</code></pre> <p>Log the hardware and software environment.</p> <p>Parameters:</p> <ul> <li> level             (<code>int</code>)         \u2013 <p>Logging level.</p> </li> <li> ranks             (<code>Iterable[int] | None</code>)         \u2013 <p>Ranks to log the environment on. If <code>None</code>, logs on all ranks.</p> </li> </ul> Source code in <code>llm/environment.py</code> <pre><code>def log_environment(\nlevel: int = logging.INFO,\nranks: Iterable[int] | None = (0,),\n) -&gt; None:\n\"\"\"Log the hardware and software environment.\n    Args:\n        level: Logging level.\n        ranks: Ranks to log the environment on. If `None`, logs on all ranks.\n    \"\"\"\nenv = collect_environment()\nenv_str = ENVIRONMENT_FORMAT.format(**env._asdict())\nlogger.log(\nlevel,\nf'Runtime environment:\\n{env_str}',\nextra={'ranks': ranks},\n)\n</code></pre>"},{"location":"api/initialize/","title":"llm.initialize","text":"<code>llm/initialize.py</code> <p>Utilities for initializing training environments.</p> <p>These utilities are used by the training scripts in <code>llm.trainers</code>.</p>"},{"location":"api/initialize/#llm.initialize.get_default_parser","title":"get_default_parser()","text":"<pre><code>get_default_parser(\nprog: str | None = None,\ndescription: str | None = None,\nusage: str | None = None,\n) -&gt; argparse.ArgumentParser\n</code></pre> <p>Get the default argument parser to be used with <code>initialize_from_args()</code>.</p> <p>Parameters:</p> <ul> <li> prog             (<code>str | None</code>)         \u2013 <p>Optional name of the program.</p> </li> <li> description             (<code>str | None</code>)         \u2013 <p>Optional description of the program.</p> </li> <li> usage             (<code>str | None</code>)         \u2013 <p>Optional program usage.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>argparse.ArgumentParser</code>         \u2013 <p>Parser which you can append your own arguments to.</p> </li> </ul> Source code in <code>llm/initialize.py</code> <pre><code>def get_default_parser(\nprog: str | None = None,\ndescription: str | None = None,\nusage: str | None = None,\n) -&gt; argparse.ArgumentParser:\n\"\"\"Get the default argument parser to be used with [`initialize_from_args()`][llm.initialize.initialize_from_args].\n    Args:\n        prog: Optional name of the program.\n        description: Optional description of the program.\n        usage: Optional program usage.\n    Returns:\n        Parser which you can append your own arguments to.\n    \"\"\"  # noqa: E501\nparser = argparse.ArgumentParser(\nprog=prog,\ndescription=description,\nusage=usage,\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n)\nparser.add_argument('--config', help='path to config file')\nparser.add_argument(\n'--debug',\naction='store_true',\nhelp='single worker distributed configuration for debugging',\n)\nparser.add_argument(\n'--loglevel',\nchoices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],\ndefault='INFO',\nhelp='minimum logging level',\n)\nparser.add_argument(\n'--rich',\naction='store_true',\nhelp='use rich for pretty stdout logging',\n)\nreturn parser\n</code></pre>"},{"location":"api/initialize/#llm.initialize.initialize","title":"initialize()","text":"<pre><code>initialize(\n*,\ndebug: bool = False,\nloglevel: int | str = \"INFO\",\nlogfile: pathlib.Path | str | None = None,\nseed: int | None = None,\nrich: bool = False\n) -&gt; None\n</code></pre> <p>Initialize the distributed context.</p> <p>Perform the following: 1) initialize logging, 2) initialized torch distributed, and 3) set the cuda device is available.</p> <p>Parameters:</p> <ul> <li> debug             (<code>bool</code>)         \u2013 <p>Initialize torch distributed for debugging (single worker).</p> </li> <li> loglevel             (<code>int | str</code>)         \u2013 <p>Minimum logging level.</p> </li> <li> logfile             (<code>pathlib.Path | str | None</code>)         \u2013 <p>Log filepath.</p> </li> <li> seed             (<code>int | None</code>)         \u2013 <p>Random seed.</p> </li> <li> rich             (<code>bool</code>)         \u2013 <p>Use rich formatting for stdout logging.</p> </li> </ul> Source code in <code>llm/initialize.py</code> <pre><code>def initialize(\n*,\ndebug: bool = False,\nloglevel: int | str = 'INFO',\nlogfile: pathlib.Path | str | None = None,\nseed: int | None = None,\nrich: bool = False,\n) -&gt; None:\n\"\"\"Initialize the distributed context.\n    Perform the following: 1) initialize logging, 2) initialized torch\n    distributed, and 3) set the cuda device is available.\n    Args:\n        debug: Initialize torch distributed for debugging (single worker).\n        loglevel: Minimum logging level.\n        logfile: Log filepath.\n        seed: Random seed.\n        rich: Use rich formatting for stdout logging.\n    \"\"\"\ninit_logging(loglevel, logfile=logfile, rich=rich, distributed=True)\nbackend = 'nccl' if torch.cuda.is_available() else 'gloo'\nif debug:\nos.environ['LOCAL_RANK'] = '0'\nos.environ['MASTER_ADDR'] = 'localhost'\nos.environ['MASTER_PORT'] = '29501'\ntorch.distributed.init_process_group(\nbackend=backend,\nworld_size=1,\nrank=0,\n)\nelse:\ntorch.distributed.init_process_group(backend=backend)\nlogger.info(\n'Distributed initialization complete: '\nf'backend={torch.distributed.get_backend()}, '\nf'world_size={torch.distributed.get_world_size()}',\nextra={'ranks': [0]},\n)\nif torch.cuda.is_available():\nlocal_rank = int(os.environ['LOCAL_RANK'])\ntorch.cuda.set_device(local_rank)\nlogger.info(\nf'Initialized CUDA local device to {local_rank}',\nextra={'ranks': [0]},\n)\nif seed is not None:\nlocal_rank = int(os.environ.get('LOCAL_RANK', 0))\nrandom.seed(seed + local_rank)\nnumpy.random.seed(seed + local_rank)\ntorch.manual_seed(seed + local_rank)\nif torch.cuda.is_available():\ntorch.cuda.manual_seed(seed + local_rank)\n</code></pre>"},{"location":"api/initialize/#llm.initialize.initialize_from_args","title":"initialize_from_args()","text":"<pre><code>initialize_from_args(args: argparse.Namespace) -&gt; Config\n</code></pre> <p>Load config and initialize from args.</p> Example <pre><code>import sys\nfrom typing import Sequence\nfrom llm.initialize import get_default_parser\nfrom llm.initialize import initialize_from_args\ndef main(argv: Sequence[str] | None = None) -&gt; int:\nargv = argv if argv is not None else sys.argv[1:]\nparser = get_default_parser()\nargs = parser.parse_args(argv)\nconfig = initialize_from_args(args)\n# Rest of your training script\nif __name__ == '__main__':\nraise SystemExit(main())\n</code></pre> Source code in <code>llm/initialize.py</code> <pre><code>def initialize_from_args(args: argparse.Namespace) -&gt; Config:\n\"\"\"Load config and initialize from args.\n    Example:\n        ```python\n        import sys\n        from typing import Sequence\n        from llm.initialize import get_default_parser\n        from llm.initialize import initialize_from_args\n        def main(argv: Sequence[str] | None = None) -&gt; int:\n            argv = argv if argv is not None else sys.argv[1:]\n            parser = get_default_parser()\n            args = parser.parse_args(argv)\n            config = initialize_from_args(args)\n            # Rest of your training script\n        if __name__ == '__main__':\n            raise SystemExit(main())\n        ```\n    \"\"\"\nconfig = load_config(args.config)\ninitialize(\ndebug=args.debug,\nloglevel=args.loglevel or config.get('LOG_LEVEL', None),\nlogfile=config.get('LOG_FILE', None),\nseed=config.get('SEED', None),\nrich=args.rich,\n)\nreturn config\n</code></pre>"},{"location":"api/loss/","title":"llm.loss","text":"<code>llm/loss.py</code> <p>Training loss functions.</p>"},{"location":"api/loss/#llm.loss.BertPretrainingCriterion","title":"BertPretrainingCriterion","text":"<pre><code>BertPretrainingCriterion(vocab_size: int) -&gt; None\n</code></pre> <p>         Bases: <code>torch.nn.Module</code></p> <p>BERT pretraining loss.</p> <p>Computes the sum of the cross entropy losses of the masked language model and (optionally) next sentence prediction tasks.</p> <p>Parameters:</p> <ul> <li> vocab_size             (<code>int</code>)         \u2013 <p>Size of the pretraining vocabulary.</p> </li> </ul> Source code in <code>llm/loss.py</code> <pre><code>def __init__(self, vocab_size: int) -&gt; None:\nsuper().__init__()\nself.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-1)\nself.vocab_size = vocab_size\n</code></pre>"},{"location":"api/loss/#llm.loss.BertPretrainingCriterion.forward","title":"forward()","text":"<pre><code>forward(\nprediction_scores: torch.FloatTensor,\nmasked_lm_labels: torch.LongTensor,\nseq_relationship_score: torch.FloatTensor | None = None,\nnext_sentence_labels: torch.LongTensor | None = None,\n) -&gt; float\n</code></pre> <p>Compute the pretraining loss.</p> <p>Parameters:</p> <ul> <li> prediction_scores             (<code>torch.FloatTensor</code>)         \u2013 <p>Masked token predictions.</p> </li> <li> masked_lm_labels             (<code>torch.LongTensor</code>)         \u2013 <p>True masked token labels.</p> </li> <li> seq_relationship_score             (<code>torch.FloatTensor | None</code>)         \u2013 <p>Predicted sequence relationship score.</p> </li> <li> next_sentence_labels             (<code>torch.LongTensor | None</code>)         \u2013 <p>True next sentence label.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013 <p>Computed loss.</p> </li> </ul> Source code in <code>llm/loss.py</code> <pre><code>def forward(\nself,\nprediction_scores: torch.FloatTensor,\nmasked_lm_labels: torch.LongTensor,\nseq_relationship_score: torch.FloatTensor | None = None,\nnext_sentence_labels: torch.LongTensor | None = None,\n) -&gt; float:\n\"\"\"Compute the pretraining loss.\n    Args:\n        prediction_scores: Masked token predictions.\n        masked_lm_labels: True masked token labels.\n        seq_relationship_score: Predicted sequence relationship score.\n        next_sentence_labels: True next sentence label.\n    Returns:\n        Computed loss.\n    \"\"\"\nmasked_lm_loss = self.loss_fn(\nprediction_scores.view(-1, self.vocab_size),\nmasked_lm_labels.view(-1),\n)\nif (\nseq_relationship_score is not None\nand next_sentence_labels is not None\n):\nnext_sentence_loss = self.loss_fn(\nseq_relationship_score.view(-1, 2),\nnext_sentence_labels.view(-1),\n)\nmasked_lm_loss += next_sentence_loss\nreturn masked_lm_loss\n</code></pre>"},{"location":"api/optimizers/","title":"llm.optimizers","text":"<code>llm/optimizers.py</code> <p>Training optimizers.</p>"},{"location":"api/optimizers/#llm.optimizers.get_optimizer","title":"get_optimizer()","text":"<pre><code>get_optimizer(\nname: Literal[\"lamb\", \"adam\"],\nparams: Iterable[torch.Tensor]\n| Iterable[dict[str, Any]],\nlr: float,\n**kwargs: Any\n) -&gt; torch.optim.Optimizer\n</code></pre> <p>Get an optimizer by name.</p> Note <p>If ColossalAI is installed, fused versions of the optimizers will be created instead.</p> <p>Parameters:</p> <ul> <li> name             (<code>Literal['lamb', 'adam']</code>)         \u2013 <p>Name of the optimizer to load.</p> </li> <li> params             (<code>Iterable[torch.Tensor] | Iterable[dict[str, Any]]</code>)         \u2013 <p>Parameters to be optimized.</p> </li> <li> lr             (<code>float</code>)         \u2013 <p>Learning rate.</p> </li> <li> kwargs             (<code>Any</code>)         \u2013 <p>Keyword arguments to pass to the optimizer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.optim.Optimizer</code>         \u2013 <p>Initialized optimizer.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>           \u2013         <p>If <code>name=='lamb'</code> and ColossalAI is not installed.</p> </li> <li> <code>ValueError</code>           \u2013         <p>If <code>name</code> is unknown.</p> </li> </ul> Source code in <code>llm/optimizers.py</code> <pre><code>def get_optimizer(\nname: Literal['lamb', 'adam'],\nparams: Iterable[torch.Tensor] | Iterable[dict[str, Any]],\nlr: float,\n**kwargs: Any,\n) -&gt; torch.optim.Optimizer:\n\"\"\"Get an optimizer by name.\n    Note:\n        If [ColossalAI](https://github.com/hpcaitech/ColossalAI){target=_blank}\n        is installed, fused versions of the optimizers will be created instead.\n    Args:\n        name: Name of the optimizer to load.\n        params: Parameters to be optimized.\n        lr: Learning rate.\n        kwargs: Keyword arguments to pass to the optimizer.\n    Returns:\n        Initialized optimizer.\n    Raises:\n        ImportError: If `name=='lamb'` and ColossalAI is not installed.\n        ValueError: If `name` is unknown.\n    \"\"\"\nif name == 'adam':  # pragma: no cover\nif FUSED_IMPORT_ERROR is None:\noptimizer = FusedAdam(params, lr=lr, **kwargs)\nelse:\nlogger.warning(\n'ColossalAI with CUDA extensions is not installed so '\n'defaulting to native PyTorch Adam. Better performance can be '\n'enabled with ColossalAI\\'s FusedAdam.',\n)\noptimizer = torch.optim.Adam(params, lr=lr, **kwargs)\nelif name == 'lamb':  # pragma: no cover\nif FUSED_IMPORT_ERROR is None:\noptimizer = FusedLAMB(params, lr=lr, **kwargs)\nelse:\nraise ImportError(\n'FusedLamb is not available. ColossalAI with CUDA extensions '\n'is not installed.',\n) from FUSED_IMPORT_ERROR\nelse:\nraise ValueError(f'Unknown optimizer: {name}')\nreturn optimizer\n</code></pre>"},{"location":"api/schedulers/","title":"llm.schedulers","text":"<code>llm/schedulers.py</code> <p>Custom learning rate schedules.</p>"},{"location":"api/schedulers/#llm.schedulers.LinearWarmupLR","title":"LinearWarmupLR","text":"<pre><code>LinearWarmupLR(\noptimizer: torch.optim.Optimizer,\ntotal_steps: int,\nwarmup_steps: int = 0,\nlast_epoch: int = -1,\n) -&gt; None\n</code></pre> <p>         Bases: <code>_LRScheduler</code></p> <p>Linear warmup and decay LR scheduler.</p> <p>Source: ColossalAI</p> <p>Parameters:</p> <ul> <li> optimizer             (<code>torch.optim.Optimizer</code>)         \u2013 <p>Optimizer to adjust learning rate of.</p> </li> <li> total_steps             (<code>int</code>)         \u2013 <p>Total training steps.</p> </li> <li> warmup_steps             (<code>int</code>)         \u2013 <p>Steps to linearly warmup the learning rate.</p> </li> <li> last_epoch             (<code>int</code>)         \u2013 <p>Optional last epoch.</p> </li> </ul> Source code in <code>llm/schedulers.py</code> <pre><code>def __init__(\nself,\noptimizer: torch.optim.Optimizer,\ntotal_steps: int,\nwarmup_steps: int = 0,\nlast_epoch: int = -1,\n) -&gt; None:\nself.total_steps = total_steps\nself.warmup_steps = warmup_steps\nsuper().__init__(optimizer, last_epoch)\n</code></pre>"},{"location":"api/schedulers/#llm.schedulers.LinearWarmupLR.get_lr","title":"get_lr()","text":"<pre><code>get_lr() -&gt; list[float]\n</code></pre> <p>Compute the current learning rate.</p> Source code in <code>llm/schedulers.py</code> <pre><code>def get_lr(self) -&gt; list[float]:\n\"\"\"Compute the current learning rate.\"\"\"\nif self.last_epoch &lt; self.warmup_steps:\nfactor = (self.last_epoch + 1) / (self.warmup_steps + 1)\nreturn [base_lr * factor for base_lr in self.base_lrs]\nelse:\nfactor = self.total_steps - self.last_epoch\nfactor = factor / (self.total_steps - self.warmup_steps)\nreturn [base_lr * factor for base_lr in self.base_lrs]\n</code></pre>"},{"location":"api/timer/","title":"llm.timer","text":"<code>llm/timer.py</code> <p>Performance timer.</p>"},{"location":"api/timer/#llm.timer.Timer","title":"Timer","text":"<pre><code>Timer(synchronize: bool = False) -&gt; None\n</code></pre> <p>Performance timer.</p> <p>Parameters:</p> <ul> <li> synchronize             (<code>bool</code>)         \u2013 <p>Synchronize CUDA workers before and after the timer.</p> </li> </ul> Source code in <code>llm/timer.py</code> <pre><code>def __init__(self, synchronize: bool = False) -&gt; None:\nself._synchronize = synchronize\nself._history: list[float] = []\nself._start_time = time.time()\n</code></pre>"},{"location":"api/timer/#llm.timer.Timer.start","title":"start()","text":"<pre><code>start() -&gt; None\n</code></pre> <p>Start the timer.</p> Source code in <code>llm/timer.py</code> <pre><code>def start(self) -&gt; None:\n\"\"\"Start the timer.\"\"\"\nif torch.cuda.is_available() and self._synchronize:\ntorch.cuda.synchronize()\nself._start_time = time.time()\n</code></pre>"},{"location":"api/timer/#llm.timer.Timer.stop","title":"stop()","text":"<pre><code>stop() -&gt; float\n</code></pre> <p>Stop the timer and return the elapsed time.</p> Source code in <code>llm/timer.py</code> <pre><code>def stop(self) -&gt; float:\n\"\"\"Stop the timer and return the elapsed time.\"\"\"\nif torch.cuda.is_available() and self._synchronize:\ntorch.cuda.synchronize()\nelapsed = time.time() - self._start_time\nself._history.append(elapsed)\nreturn elapsed\n</code></pre>"},{"location":"api/timer/#llm.timer.Timer.lap","title":"lap()","text":"<pre><code>lap() -&gt; float\n</code></pre> <p>Log a lap of the timer.</p> Source code in <code>llm/timer.py</code> <pre><code>def lap(self) -&gt; float:\n\"\"\"Log a lap of the timer.\"\"\"\nlap_time = self.stop()\nself.start()\nreturn lap_time\n</code></pre>"},{"location":"api/timer/#llm.timer.Timer.get_history","title":"get_history()","text":"<pre><code>get_history() -&gt; list[float]\n</code></pre> <p>Get the history of all lap times.</p> Source code in <code>llm/timer.py</code> <pre><code>def get_history(self) -&gt; list[float]:\n\"\"\"Get the history of all lap times.\"\"\"\nreturn self._history\n</code></pre>"},{"location":"api/timer/#llm.timer.Timer.get_history_sum","title":"get_history_sum()","text":"<pre><code>get_history_sum() -&gt; float\n</code></pre> <p>Get the sum of all lap times.</p> Source code in <code>llm/timer.py</code> <pre><code>def get_history_sum(self) -&gt; float:\n\"\"\"Get the sum of all lap times.\"\"\"\nreturn sum(self._history)\n</code></pre>"},{"location":"api/timer/#llm.timer.Timer.get_history_mean","title":"get_history_mean()","text":"<pre><code>get_history_mean() -&gt; float\n</code></pre> <p>Get the mean of all lap times.</p> Source code in <code>llm/timer.py</code> <pre><code>def get_history_mean(self) -&gt; float:\n\"\"\"Get the mean of all lap times.\"\"\"\nreturn sum(self._history) / len(self._history)\n</code></pre>"},{"location":"api/utils/","title":"llm.utils","text":"<code>llm/utils.py</code>"},{"location":"api/utils/#llm.utils.HParamT","title":"HParamT  <code>module-attribute</code>","text":"<pre><code>HParamT = Union[bool, float, int, str, None]\n</code></pre> <p>Supported Hyperparameter types (i.e., JSON types).</p>"},{"location":"api/utils/#llm.utils.DistributedFilter","title":"DistributedFilter","text":"<p>         Bases: <code>logging.Filter</code></p> <p>Custom filter that allows specifying ranks to print to.</p> Example <pre><code>logger.info('My log message', extra={'ranks': [0, 2]})\n</code></pre>"},{"location":"api/utils/#llm.utils.create_summary_writer","title":"create_summary_writer()","text":"<pre><code>create_summary_writer(\ntensorboard_dir: str,\nhparam_dict: dict[str, HParamT] | None = None,\nmetrics: list[str] | None = None,\n**writer_kwargs: Any\n) -&gt; SummaryWriter\n</code></pre> <p>Create a SummaryWriter instance for the run annotated with hyperparams.</p> <p>https://github.com/pytorch/pytorch/issues/37738#issuecomment-1124497827</p> <p>Parameters:</p> <ul> <li> tensorboard_dir             (<code>str</code>)         \u2013 <p>TensorBoard run directory.</p> </li> <li> hparam_dict             (<code>dict[str, HParamT] | None</code>)         \u2013 <p>Optional hyperparam dictionary to log alongside metrics.</p> </li> <li> metrics             (<code>list[str] | None</code>)         \u2013 <p>Optional list of metric tags that will be used with <code>writer.add_scalar()</code> (e.g., <code>['train/loss', 'train/lr']</code>). Must be provided if <code>hparam_dict</code> is provided.</p> </li> <li> writer_kwargs             (<code>Any</code>)         \u2013 <p>Additional keyword arguments to pass to <code>SummaryWriter</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SummaryWriter</code>         \u2013 <p>Summary writer instance.</p> </li> </ul> Source code in <code>llm/utils.py</code> <pre><code>def create_summary_writer(\ntensorboard_dir: str,\nhparam_dict: dict[str, HParamT] | None = None,\nmetrics: list[str] | None = None,\n**writer_kwargs: Any,\n) -&gt; SummaryWriter:\n\"\"\"Create a SummaryWriter instance for the run annotated with hyperparams.\n    https://github.com/pytorch/pytorch/issues/37738#issuecomment-1124497827\n    Args:\n        tensorboard_dir: TensorBoard run directory.\n        hparam_dict: Optional hyperparam dictionary to log alongside\n            metrics.\n        metrics: Optional list of metric tags that will be used with\n            [`writer.add_scalar()`][torch.utils.tensorboard.writer.SummaryWriter.add_scalar]\n            (e.g., `['train/loss', 'train/lr']`). Must be provided if\n            `hparam_dict` is provided.\n        writer_kwargs: Additional keyword arguments to pass to\n            `SummaryWriter`.\n    Returns:\n        Summary writer instance.\n    \"\"\"\nwriter = SummaryWriter(tensorboard_dir, **writer_kwargs)\nif hparam_dict is not None and metrics is not None:\nmetric_dict = {metric: 0 for metric in metrics}\nexp, ssi, sei = hparams(hparam_dict, metric_dict=metric_dict)\nassert writer.file_writer is not None\nwriter.file_writer.add_summary(exp)\nwriter.file_writer.add_summary(ssi)\nwriter.file_writer.add_summary(sei)\nreturn writer\n</code></pre>"},{"location":"api/utils/#llm.utils.get_filepaths","title":"get_filepaths()","text":"<pre><code>get_filepaths(\ndirectory: pathlib.Path | str,\nextensions: list[str] | None = None,\nrecursive: bool = False,\n) -&gt; list[str]\n</code></pre> <p>Get list of filepaths in directory.</p> Note <p>Only files (not sub-directories will be returned. Though sub-directories will be recursed into if <code>recursive=True</code>.</p> <p>Parameters:</p> <ul> <li> directory             (<code>pathlib.Path | str</code>)         \u2013 <p>Pathlike object with the directory to search.</p> </li> <li> extensions             (<code>list[str] | None</code>)         \u2013 <p>Pptionally only return files that match these extensions. Each extension should include the dot. E.g., <code>['.pdf', '.txt']</code>. Match is case sensitive.</p> </li> <li> recursive             (<code>bool</code>)         \u2013 <p>Recursively search sub-directories.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>         \u2013 <p>List of string paths of files in the directory.</p> </li> </ul> Source code in <code>llm/utils.py</code> <pre><code>def get_filepaths(\ndirectory: pathlib.Path | str,\nextensions: list[str] | None = None,\nrecursive: bool = False,\n) -&gt; list[str]:\n\"\"\"Get list of filepaths in directory.\n    Note:\n        Only files (not sub-directories will be returned. Though\n        sub-directories will be recursed into if `recursive=True`.\n    Args:\n        directory: Pathlike object with the directory to search.\n        extensions: Pptionally only return files that match these extensions.\n            Each extension should include the dot. E.g., `['.pdf', '.txt']`.\n            Match is case sensitive.\n        recursive: Recursively search sub-directories.\n    Returns:\n        List of string paths of files in the directory.\n    \"\"\"\ndirectory = pathlib.Path(directory)\nglob = '**/*' if recursive else '*'\nfiles = [path for path in directory.glob(glob) if path.is_file()]\nif extensions is not None:\nfiles = [path for path in files if path.suffix in extensions]\nreturn [str(path) for path in files]\n</code></pre>"},{"location":"api/utils/#llm.utils.gradient_accumulation_steps","title":"gradient_accumulation_steps()","text":"<pre><code>gradient_accumulation_steps(\nglobal_batch_size: int,\nlocal_batch_size: int,\nworld_size: int,\n) -&gt; int\n</code></pre> <p>Compute the gradient accumulation steps from the configuration.</p> <p>Parameters:</p> <ul> <li> global_batch_size             (<code>int</code>)         \u2013 <p>Target global/effective batch size.</p> </li> <li> local_batch_size             (<code>int</code>)         \u2013 <p>Per rank batch size.</p> </li> <li> world_size             (<code>int</code>)         \u2013 <p>Number of ranks.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>         \u2013 <p>Gradient accumulation steps needed to achieve the <code>global_batch_size</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013         <p>If the resulting gradient accumulation steps would be fractional.</p> </li> </ul> Source code in <code>llm/utils.py</code> <pre><code>def gradient_accumulation_steps(\nglobal_batch_size: int,\nlocal_batch_size: int,\nworld_size: int,\n) -&gt; int:\n\"\"\"Compute the gradient accumulation steps from the configuration.\n    Args:\n        global_batch_size: Target global/effective batch size.\n        local_batch_size: Per rank batch size.\n        world_size: Number of ranks.\n    Returns:\n        Gradient accumulation steps needed to achieve the `global_batch_size`.\n    Raises:\n        ValueError: If the resulting gradient accumulation steps would be\n            fractional.\n    \"\"\"\neffective_batch = local_batch_size * world_size\nif global_batch_size % effective_batch != 0:\nraise ValueError(\nf'The global batch size ({global_batch_size}) must be evenly '\n'divisible by the product of the local batch size '\nf'({local_batch_size}) and the world size ({world_size}).',\n)\nreturn global_batch_size // effective_batch\n</code></pre>"},{"location":"api/utils/#llm.utils.init_logging","title":"init_logging()","text":"<pre><code>init_logging(\nlevel: int | str = logging.INFO,\nlogfile: pathlib.Path | str | None = None,\nrich: bool = False,\ndistributed: bool = False,\n) -&gt; None\n</code></pre> <p>Configure global logging.</p> <p>Parameters:</p> <ul> <li> level             (<code>int | str</code>)         \u2013 <p>Default logging level.</p> </li> <li> logfile             (<code>pathlib.Path | str | None</code>)         \u2013 <p>Optional path to write logs to.</p> </li> <li> rich             (<code>bool</code>)         \u2013 <p>Use rich for pretty stdout logging.</p> </li> <li> distributed             (<code>bool</code>)         \u2013 <p>Configure distributed formatters and filters.</p> </li> </ul> Source code in <code>llm/utils.py</code> <pre><code>def init_logging(\nlevel: int | str = logging.INFO,\nlogfile: pathlib.Path | str | None = None,\nrich: bool = False,\ndistributed: bool = False,\n) -&gt; None:\n\"\"\"Configure global logging.\n    Args:\n        level: Default logging level.\n        logfile: Optional path to write logs to.\n        rich: Use rich for pretty stdout logging.\n        distributed: Configure distributed formatters and filters.\n    \"\"\"\nformatter = logging.Formatter(\nfmt='[%(asctime)s.%(msecs)03d] %(levelname)s (%(name)s): %(message)s',\ndatefmt='%Y-%m-%d %H:%M:%S',\n)\nstdout_handler: logging.Handler\nif rich:\nstdout_handler = RichHandler(rich_tracebacks=True)\nelse:\nstdout_handler = logging.StreamHandler(sys.stdout)\nstdout_handler.setFormatter(formatter)\nhandlers: list[logging.Handler] = [stdout_handler]\nif logfile is not None:\npath = pathlib.Path(logfile).resolve()\npath.parent.mkdir(parents=True, exist_ok=True)\nfile_handler = logging.FileHandler(path)\nfile_handler.setFormatter(formatter)\nhandlers.append(file_handler)\nif distributed:\nfilter_ = DistributedFilter()\nfor handler in handlers:\nhandler.addFilter(filter_)\nlogging.basicConfig(\nlevel=level,\nformat='%(message)s',\ndatefmt='[%X]',\nhandlers=handlers,\n)\n</code></pre>"},{"location":"api/utils/#llm.utils.log_step","title":"log_step()","text":"<pre><code>log_step(\nlogger: logging.Logger,\nstep: int,\n*,\nfmt_str: str | None = None,\nlog_level: int = logging.INFO,\nranks: Iterable[int] = (0),\nskip_tensorboard: Iterable[str] = (),\ntensorboard_prefix: str = \"train\",\nwriter: SummaryWriter | None = None,\n**kwargs: Any\n) -&gt; None\n</code></pre> <p>Log a training step.</p> <p>Parameters:</p> <ul> <li> logger             (<code>logging.Logger</code>)         \u2013 <p>Logger instance to log to.</p> </li> <li> step             (<code>int</code>)         \u2013 <p>Training step.</p> </li> <li> fmt_str             (<code>str | None</code>)         \u2013 <p>Format string used to format parameters for logging.</p> </li> <li> log_level             (<code>int</code>)         \u2013 <p>Level to log the parameters at.</p> </li> <li> ranks             (<code>Iterable[int]</code>)         \u2013 <p>Ranks to log on (default to rank 0 only).</p> </li> <li> skip_tensorboard             (<code>Iterable[str]</code>)         \u2013 <p>List of parameter names to skip logging to TensorBoard.</p> </li> <li> tensorboard_prefix             (<code>str</code>)         \u2013 <p>Prefix for TensorBoard parameters.</p> </li> <li> writer             (<code>SummaryWriter | None</code>)         \u2013 <p>TensorBoard summary writer.</p> </li> <li> kwargs             (<code>Any</code>)         \u2013 <p>Additional keyword arguments to log.</p> </li> </ul> Source code in <code>llm/utils.py</code> <pre><code>def log_step(\nlogger: logging.Logger,\nstep: int,\n*,\nfmt_str: str | None = None,\nlog_level: int = logging.INFO,\nranks: Iterable[int] = (0,),\nskip_tensorboard: Iterable[str] = (),\ntensorboard_prefix: str = 'train',\nwriter: SummaryWriter | None = None,\n**kwargs: Any,\n) -&gt; None:\n\"\"\"Log a training step.\n    Args:\n        logger: Logger instance to log to.\n        step: Training step.\n        fmt_str: Format string used to format parameters for logging.\n        log_level: Level to log the parameters at.\n        ranks: Ranks to log on (default to rank 0 only).\n        skip_tensorboard: List of parameter names to skip logging to\n            TensorBoard.\n        tensorboard_prefix: Prefix for TensorBoard parameters.\n        writer: TensorBoard summary writer.\n        kwargs: Additional keyword arguments to log.\n    \"\"\"\nvalues = {'step': step, **kwargs}\nif fmt_str is not None:\nmsg = fmt_str.format(**values)\nelse:\nmsg = ' | '.join(f'{name}: {value}' for name, value in values.items())\nlogger.log(log_level, msg, extra={'ranks': ranks})\nif writer is not None:\nfor name, value in kwargs.items():\nif name not in skip_tensorboard:\nwriter.add_scalar(f'{tensorboard_prefix}/{name}', value, step)\n</code></pre>"},{"location":"api/datasets/","title":"llm.datasets","text":"<code>llm/datasets/__init__.py</code>"},{"location":"api/datasets/bert/","title":"llm.datasets.bert","text":"<code>llm/datasets/bert.py</code> <p>NVIDIA BERT dataset provider.</p> <p>Source: https://github.com/hpcaitech/ColossalAI-Examples/blob/e0830ccc1bbc57f9c50bb1c00f3e23239bf1e231/language/roberta/pretraining/nvidia_bert_dataset_provider.py</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Batch","title":"Batch","text":"<p>         Bases: <code>NamedTuple</code></p> <p>BERT pretraining batch.</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Batch.input_ids","title":"input_ids  <code>class-attribute</code>","text":"<pre><code>input_ids: torch.LongTensor\n</code></pre> <p>Input sequence token IDs (<code>(batch_size, seq_len)</code>).</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Batch.attention_mask","title":"attention_mask  <code>class-attribute</code>","text":"<pre><code>attention_mask: torch.LongTensor\n</code></pre> <p>Input sequence attention mask (<code>(batch_size, seq_len)</code>).</p> <p>Indicates which tokens in <code>input_ids</code> should be attended to (i.e., are not padding tokens). Also known as the input mask.</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Batch.token_type_ids","title":"token_type_ids  <code>class-attribute</code>","text":"<pre><code>token_type_ids: torch.LongTensor\n</code></pre> <p>Token IDs indicating the segment labels (<code>(batch_size, seq_len)</code>).</p> <p>E.g. if <code>input_ids</code> is composed of two distinct segments, the first segment will have token IDs set to 0 and the second to 1. Also known as the segment ids.</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Batch.masked_labels","title":"masked_labels  <code>class-attribute</code>","text":"<pre><code>masked_labels: torch.LongTensor\n</code></pre> <p>True token ID of masked tokens in <code>input_ids</code> (<code>(batch_size, seq_len)</code>).</p> <p>Indices corresponding to non-masked tokens in <code>input_ids</code> are typically set to <code>-100</code> to avoid contributing to the MLM loss.</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Batch.next_sentence_labels","title":"next_sentence_labels  <code>class-attribute</code>","text":"<pre><code>next_sentence_labels: torch.LongTensor | None\n</code></pre> <p>Boolean tensor indicating the next sentence label (<code>(batch_size,)</code>).</p> <p>A true (<code>1</code>) value indicates the next sentence/segment logically follows the first. If there is only one segment in <code>input_ids</code>, this can be set to <code>None</code>.</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Sample","title":"Sample","text":"<p>         Bases: <code>NamedTuple</code></p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Sample.input_ids","title":"input_ids  <code>class-attribute</code>","text":"<pre><code>input_ids: torch.LongTensor\n</code></pre> <p>Input sequence token IDs (<code>(seq_len,)</code>).</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Sample.attention_mask","title":"attention_mask  <code>class-attribute</code>","text":"<pre><code>attention_mask: torch.LongTensor\n</code></pre> <p>Input sequence attention mask (<code>(seq_len,)</code>).</p> <p>Indicates which tokens in <code>input_ids</code> should be attended to (i.e., are not padding tokens). Also known as the input mask.</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Sample.token_type_ids","title":"token_type_ids  <code>class-attribute</code>","text":"<pre><code>token_type_ids: torch.LongTensor\n</code></pre> <p>Token IDs indicating the segment labels (<code>(seq_len,)</code>).</p> <p>E.g. if <code>input_ids</code> is composed of two distinct segments, the first segment will have token IDs set to 0 and the second to 1. Also known as the segment ids.</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Sample.masked_labels","title":"masked_labels  <code>class-attribute</code>","text":"<pre><code>masked_labels: torch.LongTensor\n</code></pre> <p>True token ID of masked tokens in <code>input_ids</code> (<code>(seq_len,)</code>).</p> <p>Indices corresponding to non-masked tokens in <code>input_ids</code> are typically set to <code>-100</code> to avoid contributing to the MLM loss.</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Sample.next_sentence_label","title":"next_sentence_label  <code>class-attribute</code>","text":"<pre><code>next_sentence_label: torch.LongTensor | None\n</code></pre> <p>Boolean scalar tensor indicating the next sentence label.</p> <p>A true (<code>1</code>) value indicates the next sentence/segment logically follows the first. If there is only one segment in <code>input_ids</code>, this can be set to <code>None</code>.</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.NvidiaBertDataset","title":"NvidiaBertDataset","text":"<pre><code>NvidiaBertDataset(input_file: str) -&gt; None\n</code></pre> <p>         Bases: <code>Dataset[Sample]</code></p> <p>NVIDIA BERT dataset.</p> <p>Like the PyTorch <code>Dataset</code>, this dataset is indexable returning a <code>Sample</code>.</p> Example <pre><code>&gt;&gt;&gt; from llm.datasets.bert import NvidiaBertDataset\n&gt;&gt;&gt; dataset = NvidiaBertDataset('/path/to/shard')\n&gt;&gt;&gt; dataset[5]\nSample(...)\n</code></pre> <p>Parameters:</p> <ul> <li> input_file             (<code>str</code>)         \u2013 <p>HDF5 file to load.</p> </li> </ul> Source code in <code>llm/datasets/bert.py</code> <pre><code>def __init__(self, input_file: str) -&gt; None:\nself.input_file = input_file\n# We just inspect the file enough to find the size of the dataset\n# then defer loading until we actually use the dataset. This is\n# particularly helpful for the DistributedShardedDataset which needs\n# to get the length of each shard\nself.loaded = False\nwith h5py.File(self.input_file, 'r') as f:\nself.samples = len(f['next_sentence_labels'])\n</code></pre>"},{"location":"api/datasets/bert/#llm.datasets.bert.get_masked_labels","title":"get_masked_labels()","text":"<pre><code>get_masked_labels(\nseq_len: int,\nmasked_lm_positions: list[int],\nmasked_lm_ids: list[int],\n) -&gt; np.ndarray[Any, Any]\n</code></pre> <p>Create masked labels array.</p> <p>Parameters:</p> <ul> <li> seq_len             (<code>int</code>)         \u2013 <p>Sequence length.</p> </li> <li> masked_lm_positions             (<code>list[int]</code>)         \u2013 <p>Index in sequence of masked tokens</p> </li> <li> masked_lm_ids             (<code>list[int]</code>)         \u2013 <p>True token value for each position in <code>masked_lm_position</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray[Any, Any]</code>         \u2013 <p>List with length <code>seq_len</code> with the true value for each corresponding         masked token in <code>input_ids</code> and -100 for all tokens in input_ids         which are not masked.</p> </li> </ul> Source code in <code>llm/datasets/bert.py</code> <pre><code>def get_masked_labels(\nseq_len: int,\nmasked_lm_positions: list[int],\nmasked_lm_ids: list[int],\n) -&gt; np.ndarray[Any, Any]:\n\"\"\"Create masked labels array.\n    Args:\n        seq_len: Sequence length.\n        masked_lm_positions: Index in sequence of masked tokens\n        masked_lm_ids: True token value for each position in\n            `masked_lm_position`.\n    Returns:\n        List with length `seq_len` with the true value for each corresponding \\\n        masked token in `input_ids` and -100 for all tokens in input_ids \\\n        which are not masked.\n    \"\"\"\nmasked_lm_labels = np.ones(seq_len, dtype=np.int64) * -100\nif len(masked_lm_positions) &gt; 0:\n# store number of  masked tokens in index\n(padded_mask_indices,) = np.nonzero(np.array(masked_lm_positions) == 0)\nif len(padded_mask_indices) != 0:\nindex = padded_mask_indices[0]\nelse:\nindex = len(masked_lm_positions)\nmasked_lm_labels[masked_lm_positions[:index]] = masked_lm_ids[:index]\nreturn masked_lm_labels\n</code></pre>"},{"location":"api/datasets/bert/#llm.datasets.bert.get_dataloader_from_nvidia_bert_shard","title":"get_dataloader_from_nvidia_bert_shard()","text":"<pre><code>get_dataloader_from_nvidia_bert_shard(\ninput_file: str,\nbatch_size: int,\n*,\nnum_replicas: int | None = None,\nrank: int | None = None,\nseed: int = 0,\nnum_workers: int = 4\n) -&gt; DataLoader[Batch]\n</code></pre> <p>Create a dataloader from a dataset shard.</p> <p>Parameters:</p> <ul> <li> input_file             (<code>str</code>)         \u2013 <p>HDF5 file to load.</p> </li> <li> batch_size             (<code>int</code>)         \u2013 <p>Size of batches yielded by the dataloader.</p> </li> <li> num_replicas             (<code>int | None</code>)         \u2013 <p>Number of processes participating in distributed training.</p> </li> <li> rank             (<code>int | None</code>)         \u2013 <p>Rank of the current process within <code>num_replicas</code>.</p> </li> <li> seed             (<code>int</code>)         \u2013 <p>Random seed used to shuffle the sampler.</p> </li> <li> num_workers             (<code>int</code>)         \u2013 <p>Number of subprocesses to use for data loading.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader[Batch]</code>         \u2013 <p>Dataloader which can be iterated over to yield         <code>Batch</code>.</p> </li> </ul> Source code in <code>llm/datasets/bert.py</code> <pre><code>def get_dataloader_from_nvidia_bert_shard(\ninput_file: str,\nbatch_size: int,\n*,\nnum_replicas: int | None = None,\nrank: int | None = None,\nseed: int = 0,\nnum_workers: int = 4,\n) -&gt; DataLoader[Batch]:\n\"\"\"Create a dataloader from a dataset shard.\n    Args:\n        input_file: HDF5 file to load.\n        batch_size: Size of batches yielded by the dataloader.\n        num_replicas: Number of processes participating in distributed training.\n        rank: Rank of the current process within `num_replicas`.\n        seed: Random seed used to shuffle the sampler.\n        num_workers: Number of subprocesses to use for data loading.\n    Returns:\n        Dataloader which can be iterated over to yield \\\n        [`Batch`][llm.datasets.bert.Batch].\n    \"\"\"\ndataset = NvidiaBertDataset(input_file)\nsampler: DistributedSampler[int] = DistributedSampler(\ndataset,\nnum_replicas=num_replicas,\nrank=rank,\nshuffle=True,\nseed=seed,\n)\ndataloader = DataLoader(\ndataset,\nsampler=sampler,\nbatch_size=batch_size,\nnum_workers=4,\nworker_init_fn=WorkerInitObj(seed),\npin_memory=True,\n)\nreturn dataloader\n</code></pre>"},{"location":"api/datasets/bert/#llm.datasets.bert.sharded_nvidia_bert_dataset","title":"sharded_nvidia_bert_dataset()","text":"<pre><code>sharded_nvidia_bert_dataset(\ninput_dir: str,\nbatch_size: int,\n*,\nnum_replicas: int | None = None,\nrank: int | None = None,\nseed: int = 0,\nnum_workers: int = 4\n) -&gt; Generator[Batch, None, None]\n</code></pre> <p>Simple generator which yields pretraining batches.</p> <p>Parameters:</p> <ul> <li> input_dir             (<code>str</code>)         \u2013 <p>Directory of HDF5 shards to load samples from.</p> </li> <li> batch_size             (<code>int</code>)         \u2013 <p>Size of batches yielded.</p> </li> <li> num_replicas             (<code>int | None</code>)         \u2013 <p>Number of processes participating in distributed training.</p> </li> <li> rank             (<code>int | None</code>)         \u2013 <p>Rank of the current process within <code>num_replicas</code>.</p> </li> <li> seed             (<code>int</code>)         \u2013 <p>Random seed used to shuffle the sampler.</p> </li> <li> num_workers             (<code>int</code>)         \u2013 <p>Number of subprocesses to use for data loading.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Generator[Batch, None, None]</code>         \u2013 <p>Batches of pretraining data.</p> </li> </ul> Source code in <code>llm/datasets/bert.py</code> <pre><code>def sharded_nvidia_bert_dataset(\ninput_dir: str,\nbatch_size: int,\n*,\nnum_replicas: int | None = None,\nrank: int | None = None,\nseed: int = 0,\nnum_workers: int = 4,\n) -&gt; Generator[Batch, None, None]:\n\"\"\"Simple generator which yields pretraining batches.\n    Args:\n        input_dir: Directory of HDF5 shards to load samples from.\n        batch_size: Size of batches yielded.\n        num_replicas: Number of processes participating in distributed training.\n        rank: Rank of the current process within `num_replicas`.\n        seed: Random seed used to shuffle the sampler.\n        num_workers: Number of subprocesses to use for data loading.\n    Yields:\n        Batches of pretraining data.\n    \"\"\"\nshard_filepaths = get_filepaths(\ninput_dir,\nextensions=['.h5', '.hdf5'],\nrecursive=True,\n)\nshard_filepaths.sort()\nfor shard_filepath in shard_filepaths:\ndataloader = get_dataloader_from_nvidia_bert_shard(\nshard_filepath,\nbatch_size,\nnum_replicas=num_replicas,\nrank=rank,\nseed=seed,\n)\nfor batch in dataloader:\nyield Batch(*batch)\n</code></pre>"},{"location":"api/datasets/roberta/","title":"llm.datasets.roberta","text":"<code>llm/datasets/roberta.py</code> <p>Custom RoBERTa dataset provider.</p> <p>This is designed to work with data produced by the RoBERTa encoder preprocessing script in <code>llm.preprocess.roberta</code>.</p>"},{"location":"api/datasets/roberta/#llm.datasets.roberta.RoBERTaDataset","title":"RoBERTaDataset","text":"<pre><code>RoBERTaDataset(\ninput_file: pathlib.Path | str,\nmask_token_id: int,\nmask_token_prob: float,\nvocab_size: int,\n) -&gt; None\n</code></pre> <p>         Bases: <code>Dataset[Sample]</code></p> <p>RoBERTa pretraining dataset.</p> <p>Like the PyTorch <code>Dataset</code>, this dataset is indexable returning a <code>Sample</code>.</p> <p>Samples are randomly masked as runtime using the provided parameters. Next sentence prediction is not supported.</p> Example <pre><code>&gt;&gt;&gt; from llm.datasets.roberta import RoBERTaDataset\n&gt;&gt;&gt; dataset = RoBERTaDataset('/path/to/shard')\n&gt;&gt;&gt; dataset[5]\nSample(...)\n</code></pre> <p>Parameters:</p> <ul> <li> input_file             (<code>pathlib.Path | str</code>)         \u2013 <p>HDF5 file to load.</p> </li> <li> mask_token_id             (<code>int</code>)         \u2013 <p>ID of the mask token in the vocabulary.</p> </li> <li> mask_token_prob             (<code>float</code>)         \u2013 <p>Probability of a given token in the sample being masked.</p> </li> <li> vocab_size             (<code>int</code>)         \u2013 <p>Size of the vocabulary. Used to replace masked tokens with a random token 10% of the time.</p> </li> </ul> Source code in <code>llm/datasets/roberta.py</code> <pre><code>def __init__(\nself,\ninput_file: pathlib.Path | str,\nmask_token_id: int,\nmask_token_prob: float,\nvocab_size: int,\n) -&gt; None:\nself.input_file = input_file\nself.mask_token_id = mask_token_id\nself.mask_token_prob = mask_token_prob\nself.vocab_size = vocab_size\nself.loaded = False\nwith h5py.File(self.input_file, 'r') as f:\nself.samples = len(f['input_ids'])\n</code></pre>"},{"location":"api/datasets/roberta/#llm.datasets.roberta.bert_mask_sequence","title":"bert_mask_sequence()","text":"<pre><code>bert_mask_sequence(\ntoken_ids: torch.LongTensor,\nspecial_tokens_mask: torch.BoolTensor,\nmask_token_id: int,\nmask_token_prob: float,\nvocab_size: int,\n) -&gt; tuple[torch.LongTensor, torch.LongTensor]\n</code></pre> <p>Randomly mask a BERT training sequence.</p> <p>Source: <code>transformers/data/data_collator.py</code></p> <p>Parameters:</p> <ul> <li> token_ids             (<code>torch.LongTensor</code>)         \u2013 <p>Input sequence token IDs to mask.</p> </li> <li> special_tokens_mask             (<code>torch.BoolTensor</code>)         \u2013 <p>Mask of special tokens in the sequence which should never be masked.</p> </li> <li> mask_token_id             (<code>int</code>)         \u2013 <p>ID of the mask token in the vocabulary.</p> </li> <li> mask_token_prob             (<code>float</code>)         \u2013 <p>Probability of a given token in the sample being masked.</p> </li> <li> vocab_size             (<code>int</code>)         \u2013 <p>Size of the vocabulary. Used to replace masked tokens with a random token 10% of the time.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[torch.LongTensor, torch.LongTensor]</code>         \u2013 <p>Masked <code>token_ids</code> and the masked labels.</p> </li> </ul> Source code in <code>llm/datasets/roberta.py</code> <pre><code>def bert_mask_sequence(\ntoken_ids: torch.LongTensor,\nspecial_tokens_mask: torch.BoolTensor,\nmask_token_id: int,\nmask_token_prob: float,\nvocab_size: int,\n) -&gt; tuple[torch.LongTensor, torch.LongTensor]:\n\"\"\"Randomly mask a BERT training sequence.\n    Source: [`transformers/data/data_collator.py`](https://github.com/huggingface/transformers/blob/f7329751fe5c43365751951502c00df5a4654359/src/transformers/data/data_collator.py#L748){target=_blank}\n    Args:\n        token_ids: Input sequence token IDs to mask.\n        special_tokens_mask: Mask of special tokens in the sequence which\n            should never be masked.\n        mask_token_id: ID of the mask token in the vocabulary.\n        mask_token_prob: Probability of a given token in the sample being\n            masked.\n        vocab_size: Size of the vocabulary. Used to replace masked tokens with\n            a random token 10% of the time.\n    Returns:\n        Masked `token_ids` and the masked labels.\n    \"\"\"  # noqa: E501\nmasked_labels = cast(torch.LongTensor, token_ids.clone())\nprobability_matrix = torch.full(token_ids.shape, mask_token_prob)\nspecial_tokens_mask = special_tokens_mask.bool()\nprobability_matrix.masked_fill_(special_tokens_mask, value=0.0)\nmasked_indices = torch.bernoulli(probability_matrix).bool()\n# Set non-masked tokens to -100 so loss is only computed on masked tokens\nmasked_labels[~masked_indices] = -100\n# 80% of the time replace masked token with [MASK]\nindices_replaced = (\ntorch.bernoulli(torch.full(token_ids.shape, 0.8)).bool()\n&amp; masked_indices\n)\ntoken_ids[indices_replaced] = mask_token_id\n# 10% of the time replace masked tokens with random token\nindices_random = (\ntorch.bernoulli(torch.full(token_ids.shape, 0.5)).bool()\n&amp; masked_indices\n&amp; ~indices_replaced\n)\nrandom_words = torch.randint(vocab_size, token_ids.shape, dtype=torch.long)\ntoken_ids[indices_random] = random_words[indices_random]\n# The rest of the time (10% of the time) the masked tokens are unchanged\nreturn token_ids, masked_labels\n</code></pre>"},{"location":"api/datasets/sharded/","title":"llm.datasets.sharded","text":"<code>llm/datasets/sharded.py</code> <p>Utilities for training with sharded datasets.</p>"},{"location":"api/datasets/sharded/#llm.datasets.sharded.DistributedShardedDataset","title":"DistributedShardedDataset","text":"<pre><code>DistributedShardedDataset(\ndataset_type: type[Dataset[SampleType]],\nshard_params: dict[str, DatasetParams],\n*,\nrank: int,\nworld_size: int,\nshuffle: bool = False,\nseed: int = 0\n) -&gt; None\n</code></pre> <p>         Bases: <code>Dataset[SampleType]</code></p> <p>Dataset wrapper for sharded datasets in distributed environments.</p> <p>This class manages a set of datasets (shards) and restricts ranks to viewing a subset of the global indices across the shards. This is achieved by sorting the shards and counting the samples in each shard to compute the total number of samples then chunking those samples by rank.</p> <p>For example, if there are four ranks and eight shards of equal size, rank zero will see shards zero and one, rank two will see shards two and three, and so on. The length of an instance of this class as seen by a rank will be <code>(1 / world_size) * sum_of_samples_across_shards</code>.</p> <p>This class also ensures only one shard is loaded at a time on a rank so the full dataset is never loaded into memory at once.</p> Warning <p>When building a <code>DataLoader</code> from a <code>DistributedShardedDataset</code>, do NOT use PyTorch's <code>DistributedSampler</code>. If you want to be able to save the state of the data loader, use the <code>SequentialSampler</code> because this class already provides the support for partitioning samples across ranks. This module provides a <code>ResumableSequentialSampler</code> to enable resuming sampling from the last sampled index.</p> Note <p>Samples at the end of the last shard will be dropped to ensure each rank sees an equal number of samples.</p> Todo <ul> <li>Next shard prefetching</li> <li>Sample index shuffling within a shard</li> <li>Support shuffle shard order by epoch</li> </ul> <p>Parameters:</p> <ul> <li> dataset_type             (<code>type[Dataset[SampleType]]</code>)         \u2013 <p>Dataset type that represents a single shard. This subtype of Dataset must be a map-style dataset. Iterable-style datasets are not supported.</p> </li> <li> shard_params             (<code>dict[str, DatasetParams]</code>)         \u2013 <p>Dictionary mapping shard keys to the parameters used to initialize a <code>dataset_type</code> for the shard. The parameter type is a tuple of args and kwargs.</p> </li> <li> rank             (<code>int</code>)         \u2013 <p>Rank of this process.</p> </li> <li> world_size             (<code>int</code>)         \u2013 <p>Number of ranks sharing the dataset.</p> </li> <li> shuffle             (<code>bool</code>)         \u2013 <p>Shuffle the shard order by the shard keys. The default (<code>False</code>) sorts the shards by shard key.</p> </li> <li> seed             (<code>int</code>)         \u2013 <p>Seed used for shuffling the shard order.</p> </li> </ul> Source code in <code>llm/datasets/sharded.py</code> <pre><code>def __init__(\nself,\ndataset_type: type[Dataset[SampleType]],\nshard_params: dict[str, DatasetParams],\n*,\nrank: int,\nworld_size: int,\nshuffle: bool = False,\nseed: int = 0,\n) -&gt; None:\nif not (0 &lt;= rank &lt; world_size):\nraise ValueError(\nf'Got rank={rank} which does not satisfy 0 &lt;= rank &lt; '\nf'world_size where world_size={world_size}.',\n)\nif len(shard_params) == 0:\nraise ValueError(\n'Parameters for at least one shard must be provided.',\n)\nrandom.seed(seed)\nself.dataset_type = dataset_type\nself.shard_params = shard_params\nself.rank = rank\nself.world_size = world_size\nself.shuffle = shuffle\nshard_keys = sorted(shard_params.keys())\nif shuffle:\nrandom.shuffle(shard_keys)\n# Mapping of shard_key to (start_index, end_index)\nshard_indices: dict[str, tuple[int, int]] = {}\nindex = 0\nfor shard_key in shard_keys:\nshard = self.load_shard(shard_key)\nassert isinstance(shard, Sized)\nshard_indices[shard_key] = (index, index + len(shard))\nindex += len(shard)\ndel shard\n# Drop indices from last shard to make divisible by world size\nlast_shard_key = shard_keys[-1]\nlast_shard_indices = shard_indices[last_shard_key]\nshard_indices[last_shard_key] = (\nlast_shard_indices[0],\nlast_shard_indices[1] - (last_shard_indices[1] % world_size),\n)\nself.shard_keys = shard_keys\nself.shard_indices = shard_indices\nself.total_samples = shard_indices[last_shard_key][1]\nassert len(shard_keys) == len(shard_indices) == len(shard_params)\nassert len(self) * self.world_size == self.total_samples\nself._current_shard_key: str | None = None\nself._current_shard: Dataset[SampleType] | None = None\n</code></pre>"},{"location":"api/datasets/sharded/#llm.datasets.sharded.DistributedShardedDataset.rank_index_to_global_index","title":"rank_index_to_global_index()","text":"<pre><code>rank_index_to_global_index(rank_index: int) -&gt; int\n</code></pre> <p>Convert an index local to a rank to a global index.</p> Source code in <code>llm/datasets/sharded.py</code> <pre><code>def rank_index_to_global_index(self, rank_index: int) -&gt; int:\n\"\"\"Convert an index local to a rank to a global index.\"\"\"\nrank_start_index = len(self) * self.rank\nreturn rank_start_index + rank_index\n</code></pre>"},{"location":"api/datasets/sharded/#llm.datasets.sharded.DistributedShardedDataset.rank_index_to_shard_index","title":"rank_index_to_shard_index()","text":"<pre><code>rank_index_to_shard_index(\nrank_index: int,\n) -&gt; tuple[str, int]\n</code></pre> <p>Convert an index local to a rank to a shard and shard index.</p> <p>Parameters:</p> <ul> <li> rank_index             (<code>int</code>)         \u2013 <p>Dataset index local to the rank.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[str, int]</code>         \u2013 <p>Tuple of the shard key and the index within the shard that             <code>rank_index</code> corresponds to.</p> </li> </ul> Source code in <code>llm/datasets/sharded.py</code> <pre><code>def rank_index_to_shard_index(self, rank_index: int) -&gt; tuple[str, int]:\n\"\"\"Convert an index local to a rank to a shard and shard index.\n    Args:\n        rank_index: Dataset index local to the rank.\n    Returns:\n        Tuple of the shard key and the index within the shard that \\\n        `rank_index` corresponds to.\n    \"\"\"\nglobal_index = self.rank_index_to_global_index(rank_index)\nfor shard_key in self.shard_keys:\nshard_indices = self.shard_indices[shard_key]\nif shard_indices[0] &lt;= global_index &lt; shard_indices[1]:\nreturn (shard_key, global_index - shard_indices[0])\nraise AssertionError(\nf'Rank index {rank_index} for rank {self.rank} maps to global '\nf'index {global_index} which exceeds the total samples in the '\nf'dataset ({self.total_samples}).',\n)\n</code></pre>"},{"location":"api/datasets/sharded/#llm.datasets.sharded.ResumableSequentialSampler","title":"ResumableSequentialSampler","text":"<pre><code>ResumableSequentialSampler(\ndata_source: Sized, start_index: int = 0\n) -&gt; None\n</code></pre> <p>         Bases: <code>torch.utils.data.Sampler[int]</code></p> <p>Resumable sequential sampler.</p> <p>Parameters:</p> <ul> <li> data_source             (<code>Sized</code>)         \u2013 <p>Dataset to sample sequentially from.</p> </li> <li> start_index             (<code>int</code>)         \u2013 <p>Index to resume sequential sampling from.</p> </li> </ul> Source code in <code>llm/datasets/sharded.py</code> <pre><code>def __init__(self, data_source: Sized, start_index: int = 0) -&gt; None:\nself.data_length = len(data_source)\nself.start_index = start_index\nself.index = start_index\n</code></pre>"},{"location":"api/engine/","title":"llm.engine","text":"<code>llm/engine/__init__.py</code> <p>Training engine utilities.</p> <p>This module provides wrappers for models, optimizers, and more to enable efficient training with mixed precision, distributed data parallelism, and more.</p>"},{"location":"api/engine/accumulation/","title":"llm.engine.accumulation","text":"<code>llm/engine/accumulation.py</code> <p>Utilities for easy gradient accumulation training.</p>"},{"location":"api/engine/accumulation/#llm.engine.accumulation.GradientAccumulationOptimizer","title":"GradientAccumulationOptimizer","text":"<pre><code>GradientAccumulationOptimizer(\noptimizer: BaseOptimizer,\nmodel: torch.nn.Module,\naccumulation_steps: int,\n) -&gt; None\n</code></pre> <p>         Bases: <code>BaseOptimizer</code></p> <p>Optimizer wrapper for enabling gradient accumulation.</p> <p>This wrapper will skip calls to <code>BaseOptimizer.step()</code> until <code>accumulation_steps</code> forward/backward passes have been performed.</p> <p>Parameters:</p> <ul> <li> optimizer             (<code>BaseOptimizer</code>)         \u2013 <p>Optimizer to wrap.</p> </li> <li> model             (<code>torch.nn.Module</code>)         \u2013 <p>Model being optimized.</p> </li> <li> accumulation_steps             (<code>int</code>)         \u2013 <p>Number of iterations between optimization steps.</p> </li> </ul> Source code in <code>llm/engine/accumulation.py</code> <pre><code>def __init__(\nself,\noptimizer: BaseOptimizer,\nmodel: torch.nn.Module,\naccumulation_steps: int,\n) -&gt; None:\nsuper().__init__(optimizer)\nself._accumulation_steps = accumulation_steps\nself._accumulation_step = 0\nself._model = model\n</code></pre>"},{"location":"api/engine/accumulation/#llm.engine.accumulation.GradientAccumulationOptimizer.accumulation_boundary","title":"accumulation_boundary()","text":"<pre><code>accumulation_boundary() -&gt; bool\n</code></pre> <p>Return if the current step is an accumulation boundary.</p> <p>I.e., the last call to <code>step()</code> resulted in an optimization step and no accumulation for the next step has started.</p> Source code in <code>llm/engine/accumulation.py</code> <pre><code>def accumulation_boundary(self) -&gt; bool:\n\"\"\"Return if the current step is an accumulation boundary.\n    I.e., the last call to\n    [`step()`][llm.engine.accumulation.GradientAccumulationOptimizer.step]\n    resulted in an optimization step and no accumulation for the next step\n    has started.\n    \"\"\"\nreturn self._accumulation_step == 0\n</code></pre>"},{"location":"api/engine/accumulation/#llm.engine.accumulation.GradientAccumulationOptimizer.backward","title":"backward()","text":"<pre><code>backward(loss: torch.Tensor) -&gt; None\n</code></pre> <p>Perform a backward pass.</p> Note <p>If <code>model</code> is a <code>DistributedDataParallel</code> instance, backward passes will be performed with <code>no_sync()</code> during gradient accumulation steps.</p> <p>Parameters:</p> <ul> <li> loss             (<code>torch.Tensor</code>)         \u2013 <p>Loss to compute gradients with respect to.</p> </li> </ul> Source code in <code>llm/engine/accumulation.py</code> <pre><code>def backward(self, loss: torch.Tensor) -&gt; None:\n\"\"\"Perform a backward pass.\n    Note:\n        If `model` is a\n        [`DistributedDataParallel`][torch.nn.parallel.DistributedDataParallel]\n        instance, backward passes will be performed with\n        [`no_sync()`][torch.nn.parallel.DistributedDataParallel.no_sync]\n        during gradient accumulation steps.\n    Args:\n        loss: Loss to compute gradients with respect to.\n    \"\"\"\nself._accumulation_step += 1\ncontext = (\nself._model.no_sync()\nif (\nself._accumulation_step &lt; self._accumulation_steps\nand isinstance(self._model, DistributedDataParallel)\n)\nelse contextlib.nullcontext()\n)\nwith context:\nscaled_loss = loss / self._accumulation_steps\nscaled_loss.backward()\n</code></pre>"},{"location":"api/engine/accumulation/#llm.engine.accumulation.GradientAccumulationOptimizer.step","title":"step()","text":"<pre><code>step(*args: Any, **kwargs: Any) -&gt; None\n</code></pre> <p>Perform an optimization step.</p> <p>This method is a no-op unless <code>accumulation_steps</code> have occurred.</p> Source code in <code>llm/engine/accumulation.py</code> <pre><code>def step(self, *args: Any, **kwargs: Any) -&gt; None:\n\"\"\"Perform an optimization step.\n    This method is a no-op unless `accumulation_steps` have occurred.\n    \"\"\"\nif self._accumulation_step == self._accumulation_steps:\nself._accumulation_step = 0\nself._optimizer.step(*args, **kwargs)\n</code></pre>"},{"location":"api/engine/accumulation/#llm.engine.accumulation.GradientAccumulationOptimizer.zero_grad","title":"zero_grad()","text":"<pre><code>zero_grad(*args: Any, **kwargs: Any) -&gt; None\n</code></pre> <p>Zero the gradients of the wrapped optimizer.</p> Source code in <code>llm/engine/accumulation.py</code> <pre><code>def zero_grad(self, *args: Any, **kwargs: Any) -&gt; None:\n\"\"\"Zero the gradients of the wrapped optimizer.\"\"\"\nif self._accumulation_step == 0:\nself._optimizer.zero_grad(*args, **kwargs)\n</code></pre>"},{"location":"api/engine/accumulation/#llm.engine.accumulation.GradientAccumulationLRScheduler","title":"GradientAccumulationLRScheduler","text":"<pre><code>GradientAccumulationLRScheduler(\nscheduler: _LRScheduler, accumulation_steps: int\n) -&gt; None\n</code></pre> <p>         Bases: <code>_LRScheduler</code></p> <p>LR scheduler wrapper that accounts for gradient accumulation.</p> <p>This wrapper allows you to call <code>scheduler.step()</code> after every forward/backward pass and will correctly skip the call if it happens during a gradient accumulation period.</p> <p>Parameters:</p> <ul> <li> scheduler             (<code>_LRScheduler</code>)         \u2013 <p>LR scheduler to wrap.</p> </li> <li> accumulation_steps             (<code>int</code>)         \u2013 <p>Number of iterations between optimization steps.</p> </li> </ul> Source code in <code>llm/engine/accumulation.py</code> <pre><code>def __init__(\nself,\nscheduler: _LRScheduler,\naccumulation_steps: int,\n) -&gt; None:\nself._accumulation_steps = accumulation_steps\nself._accumulation_step = 0\nself._scheduler = scheduler\n</code></pre>"},{"location":"api/engine/accumulation/#llm.engine.accumulation.GradientAccumulationLRScheduler.step","title":"step()","text":"<pre><code>step(epoch: int | None = None) -&gt; None\n</code></pre> <p>Update the learning rate.</p> <p>This method is a no-op unless <code>accumulation_steps</code> have occurred.</p> Source code in <code>llm/engine/accumulation.py</code> <pre><code>def step(self, epoch: int | None = None) -&gt; None:\n\"\"\"Update the learning rate.\n    This method is a no-op unless `accumulation_steps` have occurred.\n    \"\"\"\nself._accumulation_step += 1\nif self._accumulation_step == self._accumulation_steps:\nself._accumulation_step = 0\nself._scheduler.step(epoch)\n</code></pre>"},{"location":"api/engine/accumulation/#llm.engine.accumulation.initialize","title":"initialize()","text":"<pre><code>initialize(\nmodel: torch.nn.Module,\noptimizer: BaseOptimizer,\nscheduler: _LRScheduler,\naccumulation_steps: int = 1,\n) -&gt; tuple[\nGradientAccumulationOptimizer,\nGradientAccumulationLRScheduler,\n]\n</code></pre> <p>Initialize gradient accumulation training.</p> <p>Parameters:</p> <ul> <li> model             (<code>torch.nn.Module</code>)         \u2013 <p>Model being optimized.</p> </li> <li> optimizer             (<code>BaseOptimizer</code>)         \u2013 <p>Optimizer to wrap.</p> </li> <li> scheduler             (<code>_LRScheduler</code>)         \u2013 <p>LR scheduler to wrap.</p> </li> <li> accumulation_steps             (<code>int</code>)         \u2013 <p>Number of iterations between optimization steps.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[GradientAccumulationOptimizer, GradientAccumulationLRScheduler]</code>         \u2013 <p>The wrapped optimizer and LR scheduler.</p> </li> </ul> Source code in <code>llm/engine/accumulation.py</code> <pre><code>def initialize(\nmodel: torch.nn.Module,\noptimizer: BaseOptimizer,\nscheduler: _LRScheduler,\naccumulation_steps: int = 1,\n) -&gt; tuple[GradientAccumulationOptimizer, GradientAccumulationLRScheduler]:\n\"\"\"Initialize gradient accumulation training.\n    Args:\n        model: Model being optimized.\n        optimizer: Optimizer to wrap.\n        scheduler: LR scheduler to wrap.\n        accumulation_steps: Number of iterations between optimization steps.\n    Returns:\n        The wrapped optimizer and LR scheduler.\n    \"\"\"\nreturn (\nGradientAccumulationOptimizer(optimizer, model, accumulation_steps),\nGradientAccumulationLRScheduler(scheduler, accumulation_steps),\n)\n</code></pre>"},{"location":"api/engine/amp/","title":"llm.engine.amp","text":"<code>llm/engine/amp.py</code> <p>Utilities for easy automatic mixed precision training.</p>"},{"location":"api/engine/amp/#llm.engine.amp.AMPCriterion","title":"AMPCriterion","text":"<pre><code>AMPCriterion(\ncriterion: torch.nn.Module, autocast: torch.autocast\n) -&gt; None\n</code></pre> <p>         Bases: <code>torch.nn.Module</code></p> <p>Wrap a loss function for AMP training.</p> <p>Parameters:</p> <ul> <li> criterion             (<code>torch.nn.Module</code>)         \u2013 <p>Loss function to wrap.</p> </li> <li> autocast             (<code>torch.autocast</code>)         \u2013 <p>Autocast context manager to compute loss inside.</p> </li> </ul> Source code in <code>llm/engine/amp.py</code> <pre><code>def __init__(\nself,\ncriterion: torch.nn.Module,\nautocast: torch.autocast,\n) -&gt; None:\nsuper().__init__()\nself._criterion = criterion\nself._autocast = autocast\n</code></pre>"},{"location":"api/engine/amp/#llm.engine.amp.AMPCriterion.forward","title":"forward()","text":"<pre><code>forward(*args: Any, **kwargs: Any) -&gt; Any\n</code></pre> <p>Compute the loss inside the autocast.</p> Source code in <code>llm/engine/amp.py</code> <pre><code>def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n\"\"\"Compute the loss inside the autocast.\"\"\"\nwith self._autocast:\nreturn self._criterion(*args, **kwargs)\n</code></pre>"},{"location":"api/engine/amp/#llm.engine.amp.AMPModel","title":"AMPModel","text":"<pre><code>AMPModel(\nmodel: torch.nn.Module, autocast: torch.autocast\n) -&gt; None\n</code></pre> <p>         Bases: <code>torch.nn.Module</code></p> <p>Wrap a model for AMP training.</p> <p>Parameters:</p> <ul> <li> model             (<code>torch.nn.Module</code>)         \u2013 <p>Model to wrap.</p> </li> <li> autocast             (<code>torch.autocast</code>)         \u2013 <p>Autocast context manager to compute loss inside.</p> </li> </ul> Source code in <code>llm/engine/amp.py</code> <pre><code>def __init__(\nself,\nmodel: torch.nn.Module,\nautocast: torch.autocast,\n) -&gt; None:\nsuper().__init__()\nself._model = model\nself._autocast = autocast\n</code></pre>"},{"location":"api/engine/amp/#llm.engine.amp.AMPModel.forward","title":"forward()","text":"<pre><code>forward(*args: Any, **kwargs: Any) -&gt; Any\n</code></pre> <p>Perform a forward pass inside the autocast.</p> Source code in <code>llm/engine/amp.py</code> <pre><code>def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n\"\"\"Perform a forward pass inside the autocast.\"\"\"\nwith self._autocast:\nreturn self._model(*args, **kwargs)\n</code></pre>"},{"location":"api/engine/amp/#llm.engine.amp.AMPOptimizer","title":"AMPOptimizer","text":"<pre><code>AMPOptimizer(\nmodel: torch.nn.Module,\noptimizer: Optimizer,\nscaler: GradScaler,\nmax_norm: float | None = None,\n) -&gt; None\n</code></pre> <p>         Bases: <code>BaseOptimizer</code></p> <p>Wrap an optimizer for AMP training.</p> <p>Parameters:</p> <ul> <li> model             (<code>torch.nn.Module</code>)         \u2013 <p>Model being optimized.</p> </li> <li> optimizer             (<code>Optimizer</code>)         \u2013 <p>Optimizer to wrap.</p> </li> <li> scaler             (<code>GradScaler</code>)         \u2013 <p>Gradient scaler.</p> </li> <li> max_norm             (<code>float | None</code>)         \u2013 <p>Optionally clip gradient norm.</p> </li> </ul> Source code in <code>llm/engine/amp.py</code> <pre><code>def __init__(\nself,\nmodel: torch.nn.Module,\noptimizer: Optimizer,\nscaler: GradScaler,\nmax_norm: float | None = None,\n) -&gt; None:\nsuper().__init__(optimizer)\nself._model = model\nself._scaler = scaler\nself._max_norm = max_norm\n</code></pre>"},{"location":"api/engine/amp/#llm.engine.amp.AMPOptimizer.state_dict","title":"state_dict()","text":"<pre><code>state_dict() -&gt; dict[str, Any]\n</code></pre> <p>Dictionary containing references to the whole state of the module.</p> <p>Includes the state of the <code>grad_scaler</code>.</p> Source code in <code>llm/engine/amp.py</code> <pre><code>def state_dict(self) -&gt; dict[str, Any]:\n\"\"\"Dictionary containing references to the whole state of the module.\n    Includes the state of the `grad_scaler`.\n    \"\"\"\nstate_dict = self._optimizer.state_dict()\nassert 'grad_scaler' not in state_dict\nstate_dict['grad_scaler'] = self._scaler.state_dict()\nreturn state_dict\n</code></pre>"},{"location":"api/engine/amp/#llm.engine.amp.AMPOptimizer.load_state_dict","title":"load_state_dict()","text":"<pre><code>load_state_dict(state_dict: dict[str, Any]) -&gt; None\n</code></pre> <p>Copy the state into this module.</p> Source code in <code>llm/engine/amp.py</code> <pre><code>def load_state_dict(self, state_dict: dict[str, Any]) -&gt; None:\n\"\"\"Copy the state into this module.\"\"\"\nscaler_state_dict = state_dict.pop('grad_scaler', None)\nif scaler_state_dict is not None:  # pragma: no branch\nself._scaler.load_state_dict(scaler_state_dict)\nself._optimizer.load_state_dict(state_dict)\n</code></pre>"},{"location":"api/engine/amp/#llm.engine.amp.AMPOptimizer.backward","title":"backward()","text":"<pre><code>backward(loss: torch.Tensor) -&gt; None\n</code></pre> <p>Perform a backward pass and correctly scale the loss.</p> Source code in <code>llm/engine/amp.py</code> <pre><code>def backward(self, loss: torch.Tensor) -&gt; None:\n\"\"\"Perform a backward pass and correctly scale the loss.\"\"\"\nself._scaler.scale(loss).backward()\n</code></pre>"},{"location":"api/engine/amp/#llm.engine.amp.AMPOptimizer.step","title":"step()","text":"<pre><code>step(closure: Callable[[], float] | None = None) -&gt; None\n</code></pre> <p>Perform an optimization using the gradient scaler.</p> Source code in <code>llm/engine/amp.py</code> <pre><code>def step(self, closure: Callable[[], float] | None = None) -&gt; None:\n\"\"\"Perform an optimization using the gradient scaler.\"\"\"\nif self._max_norm is not None:\nself._scaler.unscale_(self._optimizer)\ntorch.nn.utils.clip_grad_norm_(\nself._model.parameters(),\nself._max_norm,\n)\nself._scaler.step(self._optimizer)\nself._scaler.update()\n</code></pre>"},{"location":"api/engine/amp/#llm.engine.amp.initialize","title":"initialize()","text":"<pre><code>initialize(\nmodel: torch.nn.Module,\noptimizer: Optimizer,\ncriterion: torch.nn.Module,\ndtype: torch.dtype = torch.float16,\nmax_norm: float | None = None,\n**kwargs: Any\n) -&gt; tuple[AMPModel, AMPOptimizer, AMPCriterion]\n</code></pre> <p>Initialize AMP training.</p> <p>Parameters:</p> <ul> <li> model             (<code>torch.nn.Module</code>)         \u2013 <p>Model being optimized.</p> </li> <li> optimizer             (<code>Optimizer</code>)         \u2013 <p>Optimizer to wrap.</p> </li> <li> criterion             (<code>torch.nn.Module</code>)         \u2013 <p>Loss function to wrap.</p> </li> <li> dtype             (<code>torch.dtype</code>)         \u2013 <p>Data type to perform mixed precision in. Typically <code>torch.float16</code> or <code>torch.bfloat16</code>.</p> </li> <li> max_norm             (<code>float | None</code>)         \u2013 <p>Optionally clip gradient norm.</p> </li> <li> kwargs             (<code>Any</code>)         \u2013 <p>Additional keyword arguments to pass to the <code>GradScaler</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[AMPModel, AMPOptimizer, AMPCriterion]</code>         \u2013 <p>A tuple of the wrapped model, optimizer, and loss.</p> </li> </ul> Source code in <code>llm/engine/amp.py</code> <pre><code>def initialize(\nmodel: torch.nn.Module,\noptimizer: Optimizer,\ncriterion: torch.nn.Module,\ndtype: torch.dtype = torch.float16,\nmax_norm: float | None = None,\n**kwargs: Any,\n) -&gt; tuple[AMPModel, AMPOptimizer, AMPCriterion]:\n\"\"\"Initialize AMP training.\n    Args:\n        model: Model being optimized.\n        optimizer: Optimizer to wrap.\n        criterion: Loss function to wrap.\n        dtype: Data type to perform mixed precision in. Typically\n            `torch.float16` or `torch.bfloat16`.\n        max_norm: Optionally clip gradient norm.\n        kwargs: Additional keyword arguments to pass to the\n            [`GradScaler`][torch.cuda.amp.GradScaler].\n    Returns:\n        A tuple of the wrapped model, optimizer, and loss.\n    \"\"\"\ndevice = 'cuda' if next(model.parameters()).is_cuda else 'cpu'\nautocast = torch.autocast(device, dtype=dtype)\n# GradScaler only works on CUDA tensors so we disable on CPU\nscaler = GradScaler(**kwargs, enabled=device == 'cuda')\nreturn (\nAMPModel(model, autocast),\nAMPOptimizer(model, optimizer, scaler, max_norm),\nAMPCriterion(criterion, autocast),\n)\n</code></pre>"},{"location":"api/engine/base/","title":"llm.engine.base","text":"<code>llm/engine/base.py</code>"},{"location":"api/engine/base/#llm.engine.base.BaseOptimizer","title":"BaseOptimizer","text":"<pre><code>BaseOptimizer(optimizer: Optimizer)\n</code></pre> <p>         Bases: <code>Optimizer</code></p> <p>Base optimizer wrapper.</p> <p>Compared to a standard PyTorch <code>Optimizer</code>, this optimizer adds the <code>backward()</code> that is used instead of directly calling <code>loss.backward()</code>. This is needed so the various optimizer wrappers can adjust how the loss is computed.</p> <p>Otherwise, this optimizer behaves identically to the wrapped optimizer.</p> <p>Parameters:</p> <ul> <li> optimizer             (<code>Optimizer</code>)         \u2013 <p>Standard PyTorch optimizer to wrap.</p> </li> </ul> Source code in <code>llm/engine/base.py</code> <pre><code>def __init__(self, optimizer: Optimizer):\nself._optimizer = optimizer\n</code></pre>"},{"location":"api/engine/base/#llm.engine.base.BaseOptimizer.step","title":"step()","text":"<pre><code>step(*args: Any, **kwargs: Any) -&gt; None\n</code></pre> <p>Perform an optimization step.</p> Source code in <code>llm/engine/base.py</code> <pre><code>def step(self, *args: Any, **kwargs: Any) -&gt; None:\n\"\"\"Perform an optimization step.\"\"\"\nself._optimizer.step(*args, **kwargs)\n</code></pre>"},{"location":"api/engine/base/#llm.engine.base.BaseOptimizer.zero_grad","title":"zero_grad()","text":"<pre><code>zero_grad(*args: Any, **kwargs: Any) -&gt; None\n</code></pre> <p>Zero the gradients of optimized parameters.</p> Source code in <code>llm/engine/base.py</code> <pre><code>def zero_grad(self, *args: Any, **kwargs: Any) -&gt; None:\n\"\"\"Zero the gradients of optimized parameters.\"\"\"\nself._optimizer.zero_grad(*args, **kwargs)\n</code></pre>"},{"location":"api/engine/base/#llm.engine.base.BaseOptimizer.backward","title":"backward()","text":"<pre><code>backward(loss: torch.Tensor) -&gt; None\n</code></pre> <p>Perform a backward pass using the loss.</p> Source code in <code>llm/engine/base.py</code> <pre><code>def backward(self, loss: torch.Tensor) -&gt; None:\n\"\"\"Perform a backward pass using the loss.\"\"\"\nloss.backward()\n</code></pre>"},{"location":"api/engine/initialize/","title":"llm.engine.initialize","text":"<code>llm/engine/initialize.py</code>"},{"location":"api/engine/initialize/#llm.engine.initialize.initialize","title":"initialize()","text":"<pre><code>initialize(\nmodel: torch.nn.Module,\noptimizer: torch.optim.Optimizer,\ncriterion: torch.nn.Module,\nscheduler: torch.optim.lr_scheduler._LRScheduler,\naccumulation_steps: int = 1,\ndtype: torch.dtype | None = None,\nmax_norm: float | None = None,\n**kwargs: Any\n) -&gt; tuple[\ntorch.nn.Module,\nBaseOptimizer,\ntorch.nn.Module,\ntorch.optim.lr_scheduler._LRScheduler,\n]\n</code></pre> <p>Enable advanced training features.</p> <p>This method allows you to easily wrap your training objects with transparent wrappers that enable advanced training features.</p> <p>Parameters:</p> <ul> <li> model             (<code>torch.nn.Module</code>)         \u2013 <p>Model being trained.</p> </li> <li> optimizer             (<code>torch.optim.Optimizer</code>)         \u2013 <p>Training optimizer.</p> </li> <li> criterion             (<code>torch.nn.Module</code>)         \u2013 <p>Training loss function.</p> </li> <li> scheduler             (<code>torch.optim.lr_scheduler._LRScheduler</code>)         \u2013 <p>LR scheduler.</p> </li> <li> accumulation_steps             (<code>int</code>)         \u2013 <p>Number of forward/backward passes between optimizer steps.</p> </li> <li> dtype             (<code>torch.dtype | None</code>)         \u2013 <p>Optional data type for mixed precision training.</p> </li> <li> max_norm             (<code>float | None</code>)         \u2013 <p>Optional maximum norm of gradients to clip to.</p> </li> <li> kwargs             (<code>Any</code>)         \u2013 <p>Keyword arguments to pass to the gradient scaler.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[torch.nn.Module, BaseOptimizer, torch.nn.Module, torch.optim.lr_scheduler._LRScheduler]</code>         \u2013 <p>Tuple of the wrapped model, optimizer, loss, and scheduler.</p> </li> </ul> Source code in <code>llm/engine/initialize.py</code> <pre><code>def initialize(\nmodel: torch.nn.Module,\noptimizer: torch.optim.Optimizer,\ncriterion: torch.nn.Module,\nscheduler: torch.optim.lr_scheduler._LRScheduler,\naccumulation_steps: int = 1,\ndtype: torch.dtype | None = None,\nmax_norm: float | None = None,\n**kwargs: Any,\n) -&gt; tuple[\ntorch.nn.Module,\nBaseOptimizer,\ntorch.nn.Module,\ntorch.optim.lr_scheduler._LRScheduler,\n]:\n\"\"\"Enable advanced training features.\n    This method allows you to easily wrap your training objects with\n    transparent wrappers that enable advanced training features.\n    Args:\n        model: Model being trained.\n        optimizer: Training optimizer.\n        criterion: Training loss function.\n        scheduler: LR scheduler.\n        accumulation_steps: Number of forward/backward passes between\n            optimizer steps.\n        dtype: Optional data type for mixed precision training.\n        max_norm: Optional maximum norm of gradients to clip to.\n        kwargs: Keyword arguments to pass to the gradient scaler.\n    Returns:\n        Tuple of the wrapped model, optimizer, loss, and scheduler.\n    \"\"\"\nif torch.cuda.is_available():\nlogger.debug(\nf'Moving model to model to cuda:{torch.cuda.current_device()}.',\nextra={'ranks': [0]},\n)\nmodel.cuda()\nif torch.distributed.is_initialized():\nlocal_rank = (\nint(os.environ['LOCAL_RANK'])\nif torch.cuda.is_available()\nelse None\n)\nlogger.debug(\n'Wrapping model with DistributedDataParallel with '\nf'local_rank {local_rank}',\nextra={'ranks': [0]},\n)\nmodel = torch.nn.parallel.DistributedDataParallel(\nmodel,\ndevice_ids=[local_rank] if local_rank is not None else None,\noutput_device=local_rank,\n)\noptimizer = BaseOptimizer(optimizer)\nif dtype is not None:\nlogger.debug(\nf'Initializing model for AMP training with dtype {dtype}',\nextra={'ranks': [0]},\n)\nmodel, optimizer, criterion = amp.initialize(\nmodel=model,\noptimizer=optimizer,\ncriterion=criterion,\ndtype=dtype,\nmax_norm=max_norm,\n**kwargs,\n)\nif accumulation_steps &gt; 1:\nlogger.debug(\n'Initializing model gradient accumulation steps = '\nf'{accumulation_steps}',\nextra={'ranks': [0]},\n)\noptimizer, scheduler = accumulation.initialize(\nmodel=model,\noptimizer=optimizer,\nscheduler=scheduler,\naccumulation_steps=accumulation_steps,\n)\nreturn (model, optimizer, criterion, scheduler)\n</code></pre>"},{"location":"api/models/","title":"llm.models","text":"<code>llm/models/__init__.py</code> <p>PyTorch models.</p>"},{"location":"api/models/bert/","title":"llm.models.bert","text":"<code>llm/models/bert.py</code> <p>Utilities for loading BERT models from HuggingFace.</p> <p>Source: ColossalAI-Examples</p>"},{"location":"api/models/bert/#llm.models.bert.BERT_BASE","title":"BERT_BASE  <code>module-attribute</code>","text":"<pre><code>BERT_BASE = dict(\nattention_probs_dropout_prob=0.1,\nhidden_act=\"gelu_new\",\nhidden_dropout_prob=0.1,\nhidden_size=768,\ninitializer_range=0.02,\nintermediate_size=3072,\nmax_position_embeddings=512,\nnum_attention_heads=12,\nnum_hidden_layers=12,\ntype_vocab_size=2,\nvocab_size=30522,\n)\n</code></pre> <p>BERT-base HuggingFace configuration.</p>"},{"location":"api/models/bert/#llm.models.bert.BERT_LARGE","title":"BERT_LARGE  <code>module-attribute</code>","text":"<pre><code>BERT_LARGE = dict(\nattention_probs_dropout_prob=0.1,\nhidden_act=\"gelu_new\",\nhidden_dropout_prob=0.1,\nhidden_size=1024,\ninitializer_range=0.02,\nintermediate_size=4096,\nmax_position_embeddings=512,\nnum_attention_heads=16,\nnum_hidden_layers=24,\ntype_vocab_size=2,\nvocab_size=30522,\n)\n</code></pre> <p>BERT-large HuggingFace configuration.</p>"},{"location":"api/models/bert/#llm.models.bert.from_config","title":"from_config()","text":"<pre><code>from_config(\nconfig: dict[str, Any],\ncheckpoint_gradients: bool = False,\n) -&gt; transformers.BertForPreTraining\n</code></pre> <p>Load a BERT model from the configuration.</p> <p>Parameters:</p> <ul> <li> config             (<code>dict[str, Any]</code>)         \u2013 <p>BERT configuration.</p> </li> <li> checkpoint_gradients             (<code>bool</code>)         \u2013 <p>Enable gradient checkpointing.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>transformers.BertForPreTraining</code>         \u2013 <p>BERT model.</p> </li> </ul> Source code in <code>llm/models/bert.py</code> <pre><code>def from_config(\nconfig: dict[str, Any],\ncheckpoint_gradients: bool = False,\n) -&gt; transformers.BertForPreTraining:\n\"\"\"Load a BERT model from the configuration.\n    Args:\n        config: BERT configuration.\n        checkpoint_gradients: Enable gradient checkpointing.\n    Returns:\n        BERT model.\n    \"\"\"\nconfig = transformers.BertConfig(**config)\nmodel = transformers.BertForPreTraining(config)\nif checkpoint_gradients:\nmodel.gradient_checkpointing_enable()\nelse:\nmodel.gradient_checkpointing_disable()\nreturn model\n</code></pre>"},{"location":"api/preprocess/","title":"llm.preprocess","text":"<code>llm/preprocess/__init__.py</code> <p>Pretraining preprocessing CLIs.</p> <p>See the CLI Reference for usage instructions.</p>"},{"location":"api/preprocess/download/","title":"llm.preprocess.download","text":"<code>llm/preprocess/download.py</code> <p>Pretraining corpus downloader.</p> <pre><code>python -m llm.preprocess.download --help\n</code></pre>"},{"location":"api/preprocess/download/#llm.preprocess.download.cli","title":"cli()","text":"<pre><code>cli(\ndataset: Literal[\"wikipedia\", \"bookcorpus\"],\noutput: str,\nlog_level: str,\nrich: bool,\n) -&gt; None\n</code></pre> <p>Pretraining text downloader.</p> Source code in <code>llm/preprocess/download.py</code> <pre><code>@click.command()\n@click.option(\n'--dataset',\ntype=click.Choice(['wikipedia', 'bookscorpus'], case_sensitive=False),\nrequired=True,\nhelp='Dataset to download.',\n)\n@click.option(\n'--output',\nmetavar='DIR',\nrequired=True,\nhelp='Output directory.',\n)\n@click.option(\n'--log-level',\ndefault='INFO',\ntype=click.Choice(\n['DEBUG', 'INFO', 'WARNING', 'ERROR'],\ncase_sensitive=False,\n),\nhelp='Minimum logging level.',\n)\n@click.option(\n'--rich/--no-rich',\ndefault=False,\nhelp='Use rich output formatting.',\n)\ndef cli(\ndataset: Literal['wikipedia', 'bookcorpus'],\noutput: str,\nlog_level: str,\nrich: bool,\n) -&gt; None:\n\"\"\"Pretraining text downloader.\"\"\"\ninit_logging(log_level, rich=rich)\nif dataset == 'wikipedia':\ndownload_wikipedia(output)\nelif dataset == 'bookscorpus':\ndownload_bookscorpus(output)\nelse:\nraise AssertionError('Unreachable.')\n</code></pre>"},{"location":"api/preprocess/format/","title":"llm.preprocess.format","text":"<code>llm/preprocess/format.py</code> <p>Pretraining text formatting utilities.</p>"},{"location":"api/preprocess/format/#llm.preprocess.format.combine_document_files","title":"combine_document_files()","text":"<pre><code>combine_document_files(\nfilepaths: Iterable[pathlib.Path | str],\noutput_file: pathlib.Path | str,\n) -&gt; None\n</code></pre> <p>Combine multiple text files into one.</p> Source code in <code>llm/preprocess/format.py</code> <pre><code>def combine_document_files(\nfilepaths: Iterable[pathlib.Path | str],\noutput_file: pathlib.Path | str,\n) -&gt; None:\n\"\"\"Combine multiple text files into one.\"\"\"\noutput_file = pathlib.Path(output_file)\noutput_file.parent.mkdir(exist_ok=True)\nsent_tokenizer = get_sent_tokenizer()\nwith open(output_file, 'w') as target:\nfor filepath in filepaths:\nwith open(filepath, 'r') as f:\ndocument_lines = f.readlines()\nsentences = []\nfor line in document_lines:\nsentences.extend(sent_tokenizer(line.strip()))\ntarget.write('\\n'.join(sentences))\ntarget.write('\\n\\n')\n</code></pre>"},{"location":"api/preprocess/format/#llm.preprocess.format.get_sent_tokenizer","title":"get_sent_tokenizer()","text":"<pre><code>get_sent_tokenizer() -&gt; Callable[[str], list[str]]\n</code></pre> <p>Get a sentence tokenizer.</p> <p>Returns:</p> <ul> <li> <code>Callable[[str], list[str]]</code>         \u2013 <p>An NLTK sentence tokenizer.</p> </li> </ul> Source code in <code>llm/preprocess/format.py</code> <pre><code>def get_sent_tokenizer() -&gt; Callable[[str], list[str]]:\n\"\"\"Get a sentence tokenizer.\n    Returns:\n        An NLTK sentence tokenizer.\n    \"\"\"\ndownloader = nltk.downloader.Downloader()\nif not downloader.is_installed('punkt'):  # pragma: no cover\nnltk.download('punkt', quiet=True)\ndownload_dir = downloader.default_download_dir()\nlogger.info(f'Downloaded NLTK punkt model to {download_dir}.')\nreturn nltk.tokenize.sent_tokenize\n</code></pre>"},{"location":"api/preprocess/format/#llm.preprocess.format.read_documents_bytes","title":"read_documents_bytes()","text":"<pre><code>read_documents_bytes(\nfiles: Iterable[pathlib.Path | str]\n| pathlib.Path\n| str,\n) -&gt; list[bytes]\n</code></pre> <p>Read documents from files.</p> <p>Parameters:</p> <ul> <li> files             (<code>Iterable[pathlib.Path | str] | pathlib.Path | str</code>)         \u2013 <p>List of files containing documents separated by blank lines to read.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[bytes]</code>         \u2013 <p>List of documents where each document is the read bytestring.</p> </li> </ul> Source code in <code>llm/preprocess/format.py</code> <pre><code>def read_documents_bytes(\nfiles: Iterable[pathlib.Path | str] | pathlib.Path | str,\n) -&gt; list[bytes]:\n\"\"\"Read documents from files.\n    Args:\n        files: List of files containing documents separated by blank lines\n            to read.\n    Returns:\n        List of documents where each document is the read bytestring.\n    \"\"\"\nif not isinstance(files, Iterable):\nfiles = [files]\ndocuments: list[bytes] = []\ndocument_lines: list[bytes] = []\nfor current_file in files:\nwith open(current_file, 'rb') as f:\nfor line in f.readlines():\nline = line.strip()\nif len(line) == 0 and len(document_lines) &gt; 0:\ndocuments.append(b'\\n'.join(document_lines))\ndocument_lines = []\nelif len(line) &gt; 0:\ndocument_lines.append(line)\nif len(document_lines) &gt; 0:\ndocuments.append(b'\\n'.join(document_lines))\nreturn documents\n</code></pre>"},{"location":"api/preprocess/format/#llm.preprocess.format.write_documents","title":"write_documents()","text":"<pre><code>write_documents(\npath: pathlib.Path | str, documents: list[str]\n) -&gt; None\n</code></pre> <p>Write a list of documents to a file.</p> <p>Parameters:</p> <ul> <li> path             (<code>pathlib.Path | str</code>)         \u2013 <p>Path to write documents to.</p> </li> <li> documents             (<code>list[str]</code>)         \u2013 <p>Documents to write. Each document will be separated by a blank line.</p> </li> </ul> Source code in <code>llm/preprocess/format.py</code> <pre><code>def write_documents(path: pathlib.Path | str, documents: list[str]) -&gt; None:\n\"\"\"Write a list of documents to a file.\n    Args:\n        path: Path to write documents to.\n        documents: Documents to write. Each document will be separated by\n            a blank line.\n    \"\"\"\npath = pathlib.Path(path)\npath.parent.mkdir(exist_ok=True)\nsent_tokenizer = get_sent_tokenizer()\nwith open(path, 'w') as f:\nfor document in documents:\ndocument = document.replace('\\n', ' ')\nsentences = sent_tokenizer(document)\nf.write('\\n'.join(sentences))\nf.write('\\n\\n')\n</code></pre>"},{"location":"api/preprocess/roberta/","title":"llm.preprocess.roberta","text":"<code>llm/preprocess/roberta.py</code> <p>RoBERTa pretraining encoder.</p> <p>This implements the DOC-SENTENCES sampling strategy of RoBERTa. Samples are not pre-masked as in Devlin et al. and are rather dynamically masked at runtime as in RoBERTa. Next sentence prediction is also not used.</p> <pre><code>python -m llm.preprocess.roberta --help\n</code></pre>"},{"location":"api/preprocess/roberta/#llm.preprocess.roberta.cli","title":"cli()","text":"<pre><code>cli(\ninput: str,\noutput_dir: str,\nvocab: str,\ntokenizer: Literal[\"bpe\", \"wordpiece\"],\ncased: bool,\nmax_seq_len: int,\nshort_seq_prob: float,\nprocesses: int,\nlog_level: str,\nrich: bool,\n) -&gt; None\n</code></pre> <p>RoBERTa pre-training dataset encoder.</p> Source code in <code>llm/preprocess/roberta.py</code> <pre><code>@click.command()\n@click.option(\n'--input',\nmetavar='PATH',\nrequired=True,\nhelp='Glob of input shards to encode.',\n)\n@click.option(\n'--output-dir',\nmetavar='PATH',\nrequired=True,\nhelp='Output directory for encoded shards.',\n)\n@click.option(\n'--vocab',\nmetavar='PATH',\nrequired=True,\nhelp='Vocabulary file.',\n)\n@click.option(\n'--tokenizer',\ntype=click.Choice(['bpe', 'wordpiece'], case_sensitive=False),\nrequired=True,\nhelp='Tokenizer type.',\n)\n@click.option(\n'--cased/--uncased',\ndefault=False,\nhelp='Vocab/tokenizer is case-sensitive.',\n)\n@click.option(\n'--max-seq-len',\ntype=int,\ndefault=512,\nhelp='Maximum sequence length.',\n)\n@click.option(\n'--short-seq-prob',\ntype=float,\ndefault=0.1,\nhelp='Probablity to create shorter sequences.',\n)\n@click.option(\n'-p',\n'--processes',\ntype=int,\ndefault=4,\nhelp='Number of processes for concurrent shard encoding.',\n)\n@click.option(\n'--log-level',\ndefault='INFO',\ntype=click.Choice(\n['DEBUG', 'INFO', 'WARNING', 'ERROR'],\ncase_sensitive=False,\n),\nhelp='Minimum logging level.',\n)\n@click.option(\n'--rich/--no-rich',\ndefault=False,\nhelp='Use rich output formatting.',\n)\ndef cli(\ninput: str,  # noqa: A002\noutput_dir: str,\nvocab: str,\ntokenizer: Literal['bpe', 'wordpiece'],\ncased: bool,\nmax_seq_len: int,\nshort_seq_prob: float,\nprocesses: int,\nlog_level: str,\nrich: bool,\n) -&gt; None:\n\"\"\"RoBERTa pre-training dataset encoder.\"\"\"\ninit_logging(log_level, rich=rich)\ninput_files = glob.glob(input, recursive=True)\ntokenizer = get_tokenizer(\ntokenizer,\nvocab,\nnot cased,\npadding_options={'length': max_seq_len},\ntruncation_options={'max_length': max_seq_len},\n)\nencode_files(\ninput_files=input_files,\noutput_dir=output_dir,\ntokenizer=tokenizer,\nmax_seq_len=max_seq_len,\nshort_seq_prob=short_seq_prob,\nprocesses=processes,\n)\n</code></pre>"},{"location":"api/preprocess/shard/","title":"llm.preprocess.shard","text":"<code>llm/preprocess/shard.py</code> <p>Pretraining test sharder.</p> <pre><code>python -m llm.preprocess.shard --help\n</code></pre>"},{"location":"api/preprocess/shard/#llm.preprocess.shard.cli","title":"cli()","text":"<pre><code>cli(\ninput: str,\noutput: str,\nsize: str,\nformat: str,\nshuffle: bool,\nlog_level: str,\nrich: bool,\n) -&gt; None\n</code></pre> <p>Pre-training text sharder.</p> Source code in <code>llm/preprocess/shard.py</code> <pre><code>@click.command()\n@click.option(\n'--input',\nmetavar='PATH',\nrequired=True,\nhelp='Glob of input shards to encode.',\n)\n@click.option(\n'--output',\nmetavar='PATH',\nrequired=True,\nhelp='Output directory for encoded shards.',\n)\n@click.option(\n'--size',\nmetavar='SIZE',\nrequired=True,\nhelp='Max data size of each shard.',\n)\n@click.option(\n'--format',\ndefault='shard-{index}.txt',\nhelp='Shard name format where {index} is replaced by shard index.',\n)\n@click.option(\n'--shuffle/--no-shuffle',\ndefault=False,\nhelp='Shuffle documents before sharding.',\n)\n@click.option(\n'--log-level',\ndefault='INFO',\ntype=click.Choice(\n['DEBUG', 'INFO', 'WARNING', 'ERROR'],\ncase_sensitive=False,\n),\nhelp='Minimum logging level.',\n)\n@click.option(\n'--rich/--no-rich',\ndefault=False,\nhelp='Use rich output formatting.',\n)\ndef cli(\ninput: str,  # noqa: A002\noutput: str,\nsize: str,\nformat: str,  # noqa: A002\nshuffle: bool,\nlog_level: str,\nrich: bool,\n) -&gt; None:\n\"\"\"Pre-training text sharder.\"\"\"\ninit_logging(log_level, rich=rich)\nglob.glob(input)\nsize_bytes = readable_to_bytes(size)\nshard(input, output, format, size_bytes, shuffle)\n</code></pre>"},{"location":"api/preprocess/tokenize/","title":"llm.preprocess.tokenize","text":"<code>llm/preprocess/tokenize.py</code> <p>Tokenizer utilities.</p>"},{"location":"api/preprocess/tokenize/#llm.preprocess.tokenize.get_tokenizer","title":"get_tokenizer()","text":"<pre><code>get_tokenizer(\nkind: Literal[\"wordpiece\", \"bpe\"],\nvocab: dict[str, int] | str | None = None,\nlowercase: bool = False,\npadding_options: dict[str, Any] | None = None,\ntruncation_options: dict[str, Any] | None = None,\n**kwargs: Any\n) -&gt; (\ntokenizers.implementations.base_tokenizer.BaseTokenizer\n)\n</code></pre> <p>Get a tokenizer by name.</p> <p>Parameters:</p> <ul> <li> kind             (<code>Literal['wordpiece', 'bpe']</code>)         \u2013 <p>Tokenizer name to create.</p> </li> <li> vocab             (<code>dict[str, int] | str | None</code>)         \u2013 <p>Vocabulary file or dictionary to initialized tokenizer with.</p> </li> <li> lowercase             (<code>bool</code>)         \u2013 <p>Set the tokenizer to lowercase.</p> </li> <li> padding_options             (<code>dict[str, Any] | None</code>)         \u2013 <p>Enable padding options on the tokenizer.</p> </li> <li> truncation_options             (<code>dict[str, Any] | None</code>)         \u2013 <p>Enable truncation options on the tokenizer.</p> </li> <li> kwargs             (<code>Any</code>)         \u2013 <p>Additional arguments to pass to the tokenizer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tokenizers.implementations.base_tokenizer.BaseTokenizer</code>         \u2013 <p>A tokenizer instance.</p> </li> </ul> Source code in <code>llm/preprocess/tokenize.py</code> <pre><code>def get_tokenizer(\nkind: Literal['wordpiece', 'bpe'],\nvocab: dict[str, int] | str | None = None,\nlowercase: bool = False,\npadding_options: dict[str, Any] | None = None,\ntruncation_options: dict[str, Any] | None = None,\n**kwargs: Any,\n) -&gt; tokenizers.implementations.base_tokenizer.BaseTokenizer:\n\"\"\"Get a tokenizer by name.\n    Args:\n        kind: Tokenizer name to create.\n        vocab: Vocabulary file or dictionary to initialized tokenizer with.\n        lowercase: Set the tokenizer to lowercase.\n        padding_options: Enable padding options on the tokenizer.\n        truncation_options: Enable truncation options on the tokenizer.\n        kwargs: Additional arguments to pass to the tokenizer.\n    Returns:\n        A tokenizer instance.\n    \"\"\"\nif kind == 'wordpiece':\ntokenizer = tokenizers.BertWordPieceTokenizer(\nvocab=vocab,\nclean_text=True,\nhandle_chinese_chars=True,\nlowercase=lowercase,\n**kwargs,\n)\nelif kind == 'bpe':\ntokenizer = tokenizers.ByteLevelBPETokenizer(\nvocab=vocab,\nlowercase=lowercase,\nadd_prefix_space=True,\ntrim_offsets=True,\n**kwargs,\n)\nelse:\nraise AssertionError(f'Unsupported kind \"{kind}.\"')\nif padding_options is not None:\ntokenizer.enable_padding(**padding_options)\nif truncation_options is not None:\ntokenizer.enable_truncation(**truncation_options)\nreturn tokenizer\n</code></pre>"},{"location":"api/preprocess/utils/","title":"llm.preprocess.utils","text":"<code>llm/preprocess/utils.py</code> <p>Preprocessing script utilities.</p>"},{"location":"api/preprocess/utils/#llm.preprocess.utils.readable_to_bytes","title":"readable_to_bytes()","text":"<pre><code>readable_to_bytes(size: str) -&gt; int\n</code></pre> <p>Convert string with bytes units to the integer value of bytes.</p> <p>Source: ProxyStore</p> Example <pre><code>&gt;&gt;&gt; readable_to_bytes('1.2 KB')\n1200\n&gt;&gt;&gt; readable_to_bytes('0.6 MiB')\n629146\n</code></pre> <p>Parameters:</p> <ul> <li> size             (<code>str</code>)         \u2013 <p>String to parse for bytes size.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>         \u2013 <p>Integer number of bytes parsed from the string.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013         <p>If the input string contains more than two parts (i.e., a value and a unit).</p> </li> <li> <code>ValueError</code>           \u2013         <p>If the unit is not one of KB, MB, GB, TB, KiB, MiB, GiB, or TiB.</p> </li> <li> <code>ValueError</code>           \u2013         <p>If the value cannot be cast to a float.</p> </li> </ul> Source code in <code>llm/preprocess/utils.py</code> <pre><code>def readable_to_bytes(size: str) -&gt; int:\n\"\"\"Convert string with bytes units to the integer value of bytes.\n    Source: [ProxyStore](https://github.com/proxystore/proxystore/blob/79dfdc0fc2c5a5093cf5e57b2ecf61c48fde6773/proxystore/utils.py#L130){target=_blank}\n    Example:\n        ```python\n        &gt;&gt;&gt; readable_to_bytes('1.2 KB')\n        1200\n        &gt;&gt;&gt; readable_to_bytes('0.6 MiB')\n        629146\n        ```\n    Args:\n        size: String to parse for bytes size.\n    Returns:\n        Integer number of bytes parsed from the string.\n    Raises:\n        ValueError: If the input string contains more than two parts\n            (i.e., a value and a unit).\n        ValueError: If the unit is not one of KB, MB, GB, TB, KiB, MiB, GiB,\n            or TiB.\n        ValueError: If the value cannot be cast to a float.\n    \"\"\"  # noqa: E501\nunits_to_bytes = dict(\nb=1,\nkb=int(1e3),\nmb=int(1e6),\ngb=int(1e9),\ntb=int(1e12),\nkib=int(2**10),\nmib=int(2**20),\ngib=int(2**30),\ntib=int(2**40),\n)\n# Try casting size to value (will only work if no units)\ntry:\nreturn int(float(size))\nexcept ValueError:\npass\n# Ensure space between value and unit\nsize = re.sub(r'([a-zA-Z]+)', r' \\1', size.strip())\nparts = [s.strip() for s in size.split()]\nif len(parts) != 2:\nraise ValueError(\n'Input string \"{size}\" must contain only a value and a unit.',\n)\nvalue, unit = parts\ntry:\nvalue_size = decimal.Decimal(value)\nexcept decimal.InvalidOperation as e:\nraise ValueError(f'Unable to interpret \"{value}\" as a float.') from e\ntry:\nunit_size = units_to_bytes[unit.lower()]\nexcept KeyError as e:\nraise ValueError(f'Unknown unit type {unit}.') from e\nreturn int(value_size * unit_size)\n</code></pre>"},{"location":"api/preprocess/utils/#llm.preprocess.utils.safe_extract","title":"safe_extract()","text":"<pre><code>safe_extract(\nname: pathlib.Path | str, target: pathlib.Path | str\n) -&gt; None\n</code></pre> <p>Safely extract a tar file.</p> Note <p>This extraction method is designed to safeguard against CVE-2007-4559.</p> <p>Parameters:</p> <ul> <li> name             (<code>pathlib.Path | str</code>)         \u2013 <p>Path to tar file to extract.</p> </li> <li> target             (<code>pathlib.Path | str</code>)         \u2013 <p>Target path to extract to.</p> </li> </ul> Source code in <code>llm/preprocess/utils.py</code> <pre><code>def safe_extract(name: pathlib.Path | str, target: pathlib.Path | str) -&gt; None:\n\"\"\"Safely extract a tar file.\n    Note:\n        This extraction method is designed to safeguard against CVE-2007-4559.\n    Args:\n        name: Path to tar file to extract.\n        target: Target path to extract to.\n    \"\"\"\nname = pathlib.Path(name).absolute()\ntarget = pathlib.Path(target).absolute()\nwith tarfile.open(name, 'r:gz') as f:\n# CVE-2007-4559 input sanitation\nfor member in f.getmembers():\nmember_path = (target / member.name).resolve()\nif target not in member_path.parents:\nraise OSError(\n'Tarfile contains member that extracts outside of the '\n'target directory. This could be a path traversal attack.',\n)\nf.extractall(target)\n</code></pre>"},{"location":"api/preprocess/vocab/","title":"llm.preprocess.vocab","text":"<code>llm/preprocess/vocab.py</code> <p>Pretraining vocabulary builder.</p> <pre><code>python -m llm.preprocess.vocab --help\n</code></pre>"},{"location":"api/preprocess/vocab/#llm.preprocess.vocab.cli","title":"cli()","text":"<pre><code>cli(\ninput: str,\noutput: str,\nsize: int,\ntokenizer: Literal[\"bpe\", \"wordpiece\"],\ncased: bool,\nspecial_token: list[str],\nlog_level: str,\nrich: bool,\n) -&gt; None\n</code></pre> <p>Pre-training vocabulary builder.</p> <p>Arguments default to the standard uncased BERT with wordpiece method.</p> Source code in <code>llm/preprocess/vocab.py</code> <pre><code>@click.command()\n@click.option(\n'--input',\nmetavar='PATH',\nrequired=True,\nhelp='Glob of input text files to build vocab from.',\n)\n@click.option(\n'--output',\nmetavar='PATH',\nrequired=True,\nhelp='Output filepath for vocabulary.',\n)\n@click.option(\n'--size',\ndefault=30522,\ntype=int,\nhelp='Size of vocabulary.',\n)\n@click.option(\n'--tokenizer',\ntype=click.Choice(['bpe', 'wordpiece'], case_sensitive=False),\ndefault='wordpiece',\nhelp='Tokenizer type.',\n)\n@click.option(\n'--cased/--uncased',\ndefault=False,\nhelp='Vocab/tokenizer is case-sensitive.',\n)\n@click.option(\n'-s',\n'--special-token',\nmultiple=True,\ndefault=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'],\nhelp='Special tokens to prepend to vocab.',\n)\n@click.option(\n'--log-level',\ndefault='INFO',\ntype=click.Choice(\n['DEBUG', 'INFO', 'WARNING', 'ERROR'],\ncase_sensitive=False,\n),\nhelp='Minimum logging level.',\n)\n@click.option(\n'--rich/--no-rich',\ndefault=False,\nhelp='Use rich output formatting.',\n)\ndef cli(\ninput: str,  # noqa: A002\noutput: str,\nsize: int,\ntokenizer: Literal['bpe', 'wordpiece'],\ncased: bool,\nspecial_token: list[str],\nlog_level: str,\nrich: bool,\n) -&gt; None:\n\"\"\"Pre-training vocabulary builder.\n    Arguments default to the standard uncased BERT with wordpiece method.\n    \"\"\"\ninit_logging(log_level, rich=rich)\ninput_files = glob.glob(input)\ntrain_vocab(\ntokenizer,\ninput_files=input_files,\noutput_file=output,\nsize=size,\nlowercase=not cased,\nspecial_tokens=special_token,\n)\n</code></pre>"},{"location":"api/trainers/","title":"llm.trainers","text":"<code>llm/trainers/__init__.py</code> <p>Training scripts.</p> <p>Each submodule of <code>llm.trainers</code> is an executable module. For example, <code>llm.trainers.bert</code> can be run as: <pre><code>python -m llm.trainers.bert --help\n</code></pre></p>"},{"location":"api/trainers/bert/","title":"llm.trainers.bert","text":"<code>llm/trainers/bert/__init__.py</code> <p>BERT pretraining module.</p> <p>See the BERT Pretraining Guide.</p>"},{"location":"api/trainers/bert/main/","title":"llm.trainers.bert.main","text":"<code>llm/trainers/bert/main.py</code> <p>BERT pretraining CLI.</p>"},{"location":"api/trainers/bert/utils/","title":"llm.trainers.bert.utils","text":"<code>llm/trainers/bert/utils.py</code> <p>BERT pretraining utilities.</p>"},{"location":"api/trainers/bert/utils/#llm.trainers.bert.utils.TrainingConfig","title":"TrainingConfig  <code>dataclass</code>","text":"<p>Training configuration.</p>"},{"location":"api/trainers/bert/utils/#llm.trainers.bert.utils.parse_config","title":"parse_config()","text":"<pre><code>parse_config(config: Config) -&gt; TrainingConfig\n</code></pre> <p>Parses a config ensuring all required options are present.</p> Source code in <code>llm/trainers/bert/utils.py</code> <pre><code>def parse_config(config: Config) -&gt; TrainingConfig:\n\"\"\"Parses a config ensuring all required options are present.\"\"\"\nconfig.ACCUMULATION_STEPS = gradient_accumulation_steps(\nglobal_batch_size=config.GLOBAL_BATCH_SIZE,\nlocal_batch_size=config.BATCH_SIZE,\nworld_size=torch.distributed.get_world_size(),\n)\nfor field_name, field_type in get_type_hints(TrainingConfig).items():\nif field_name not in config:\ncontinue\ntry:\nmatch = isinstance(config[field_name], field_type)\nexcept TypeError as e:\n# Not all types (GenericAlias types like dict[str, Any]) will\n# support isinstance checks so we just log the error and skip\nmatch = True\nlogger.debug(\nf'Unable to verify config option {field_name}: {field_type}\\n'\nf'{e}',\n)\nif not match:\nraise TypeError(\nf'Expected config entry {field_name} to be type '\nf'{field_type} but got {type(config[field_name])}.',\n)\n# Only take args that are fields of TrainingConfig\nfields = {field.name for field in dataclasses.fields(TrainingConfig)}\nconfig_ = {k: v for k, v in config.items() if k in fields}\nreturn TrainingConfig(**config_)\n</code></pre>"},{"location":"api/trainers/bert/utils/#llm.trainers.bert.utils.get_dataset","title":"get_dataset()","text":"<pre><code>get_dataset(\ndirectory: pathlib.Path | str,\n) -&gt; DistributedShardedDataset[Sample]\n</code></pre> <p>Load a sharded BERT pretraining dataset.</p> Source code in <code>llm/trainers/bert/utils.py</code> <pre><code>def get_dataset(\ndirectory: pathlib.Path | str,\n) -&gt; DistributedShardedDataset[Sample]:\n\"\"\"Load a sharded BERT pretraining dataset.\"\"\"\nfiles = get_filepaths(\ndirectory,\nextensions=['.h5', '.hdf5'],\nrecursive=True,\n)\nparams: dict[str, DatasetParams] = {file: ((file,), {}) for file in files}\nreturn DistributedShardedDataset(\nNvidiaBertDataset,\nparams,\nrank=torch.distributed.get_rank(),\nworld_size=torch.distributed.get_world_size(),\n)\n</code></pre>"},{"location":"api/trainers/bert/utils/#llm.trainers.bert.utils.get_dataloader","title":"get_dataloader()","text":"<pre><code>get_dataloader(\ndataset: DistributedShardedDataset[Sample],\nsampler: torch.utils.data.Sampler[int],\nbatch_size: int,\n) -&gt; torch.utils.data.DataLoader[Sample]\n</code></pre> <p>Create a dataloader from a sharded dataset.</p> Source code in <code>llm/trainers/bert/utils.py</code> <pre><code>def get_dataloader(\ndataset: DistributedShardedDataset[Sample],\nsampler: torch.utils.data.Sampler[int],\nbatch_size: int,\n) -&gt; torch.utils.data.DataLoader[Sample]:\n\"\"\"Create a dataloader from a sharded dataset.\"\"\"\nreturn torch.utils.data.DataLoader(\ndataset,\nbatch_size=batch_size,\nshuffle=False,\nsampler=sampler,\nnum_workers=4,\npin_memory=True,\ndrop_last=True,\n)\n</code></pre>"},{"location":"api/trainers/bert/utils/#llm.trainers.bert.utils.checkpoint","title":"checkpoint()","text":"<pre><code>checkpoint(\nconfig: TrainingConfig,\nglobal_step: int,\nepoch: int,\nmodel: torch.nn.Module,\noptimizer: torch.optim.Optimizer,\nscheduler: torch.optim.lr_scheduler._LRScheduler,\nsampler_index: int = 0,\n) -&gt; None\n</code></pre> <p>Write a training checkpoint.</p> Source code in <code>llm/trainers/bert/utils.py</code> <pre><code>def checkpoint(\nconfig: TrainingConfig,\nglobal_step: int,\nepoch: int,\nmodel: torch.nn.Module,\noptimizer: torch.optim.Optimizer,\nscheduler: torch.optim.lr_scheduler._LRScheduler,\nsampler_index: int = 0,\n) -&gt; None:\n\"\"\"Write a training checkpoint.\"\"\"\nif torch.distributed.get_rank() == 0:\n# Extract from possible AMPModel\nmodel = model._model if hasattr(model, '_model') else model\n# Extract from possible DistributedDataParallel\nmodel = model.module if hasattr(model, 'module') else model\nsave_checkpoint(\ncheckpoint_dir=config.CHECKPOINT_DIR,\nglobal_step=global_step,\nmodel=model,\noptimizer=optimizer,\nscheduler=scheduler,\nepoch=epoch,\nphase=config.PHASE,\nsampler_index=sampler_index,\n)\nlogger.info(\nf'Saved checkpoint at global step {global_step}',\nextra={'ranks': [0]},\n)\n</code></pre>"},{"location":"api/trainers/bert/utils/#llm.trainers.bert.utils.load_state","title":"load_state()","text":"<pre><code>load_state(\nconfig: TrainingConfig,\nmodel: torch.nn.Module,\noptimizer: torch.optim.Optimizer,\nscheduler: torch.optim.lr_scheduler._LRScheduler,\n) -&gt; tuple[int, int, int]\n</code></pre> <p>Load the latest checkpoint if one exists.</p> <p>Returns:</p> <ul> <li> <code>int</code>         \u2013 <p>Tuple of the global step, epoch, and sampler index to resume</p> </li> <li> <code>int</code>         \u2013 <p>(or start) from.</p> </li> </ul> Source code in <code>llm/trainers/bert/utils.py</code> <pre><code>def load_state(\nconfig: TrainingConfig,\nmodel: torch.nn.Module,\noptimizer: torch.optim.Optimizer,\nscheduler: torch.optim.lr_scheduler._LRScheduler,\n) -&gt; tuple[int, int, int]:\n\"\"\"Load the latest checkpoint if one exists.\n    Returns:\n        Tuple of the global step, epoch, and sampler index to resume\n        (or start) from.\n    \"\"\"\nglobal_step = 0\nepoch = 1\nsampler_index = 0\nos.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\ncheckpoint = load_checkpoint(\nconfig.CHECKPOINT_DIR,\nmap_location='cpu',  # next(model.parameters()).device,\n)\nif checkpoint is not None:\nlogger.info(\nf'Loaded checkpoint from {checkpoint.filepath}',\nextra={'ranks': [0]},\n)\n# Load model to the model and not the AMP wrapper\nmodel = model._model if hasattr(model, '_model') else model\n# Load model to the model and not the DDP wrapper\nmodel = model.module if hasattr(model, 'module') else model\nmodel.load_state_dict(checkpoint.model_state_dict)\nif checkpoint.kwargs['phase'] == config.PHASE:\nlogger.info(\n'Checkpoint from current phase. Loading optimizer state',\nextra={'ranks': [0]},\n)\nif (  # pragma: no branch\ncheckpoint.optimizer_state_dict is not None\n):\noptimizer.load_state_dict(checkpoint.optimizer_state_dict)\nif (  # pragma: no branch\ncheckpoint.scheduler_state_dict is not None\n):\nscheduler.load_state_dict(checkpoint.scheduler_state_dict)\nglobal_step = checkpoint.global_step\nepoch = checkpoint.kwargs['epoch']\nsampler_index = (\ncheckpoint.kwargs['sampler_index']\nif 'sampler_index' in checkpoint.kwargs\nelse 0\n)\nelse:\nlogger.info(\n'Checkpoint from new phase. Resetting optimizer state',\nextra={'ranks': [0]},\n)\nelse:\nlogger.info(\n'No checkpoint found to resume from',\nextra={'ranks': [0]},\n)\nreturn global_step, epoch, sampler_index\n</code></pre>"},{"location":"api/trainers/bert/utils/#llm.trainers.bert.utils.get_optimizer_grouped_parameters","title":"get_optimizer_grouped_parameters()","text":"<pre><code>get_optimizer_grouped_parameters(\nmodel: transformers.BertForPreTraining,\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Get the parameters of the BERT model to optimizer.</p> Source code in <code>llm/trainers/bert/utils.py</code> <pre><code>def get_optimizer_grouped_parameters(\nmodel: transformers.BertForPreTraining,\n) -&gt; list[dict[str, Any]]:\n\"\"\"Get the parameters of the BERT model to optimizer.\"\"\"\n# configure the weight decay for bert models\nparams = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta', 'LayerNorm']\nparams_decay = [p for n, p in params if not any(v in n for v in no_decay)]\nparams_no_decay = [p for n, p in params if any(v in n for v in no_decay)]\nreturn [\n{'params': params_decay, 'weight_decay': 0.01},\n{'params': params_no_decay, 'weight_decay': 0.0},\n]\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#getting-started-for-local-development","title":"Getting Started for Local Development","text":"<p>We recommend using Tox to setup the development environment. This will create a new virtual environment with all of the required packages installed and <code>llm</code> installed in editable mode with the necessary extras options.</p> <pre><code>$ git clone https://github.com/gpauloski/llm-pytorch\n$ cd llm-pytorch\n$ tox --devenv venv -e py310\n$ . venv/bin/activate\n</code></pre> <p>Warning</p> <p>Running Tox in a Conda environment is possible but it may conflict with Tox's ability to find the correct Python versions. E.g., if your Conda environment is Python 3.9, running <code>$ tox -e p38</code> may still use Python 3.9.</p> <p>To install manually: <pre><code>$ git clone https://github.com/gpauloski/llm-pytorch\n$ cd llm-pytorch\n$ python -m venv venv\n$ . venv/bin/activate\n$ pip install -e .[dev,docs,colossalai]\n</code></pre></p>"},{"location":"contributing/#continuous-integration","title":"Continuous Integration","text":"<p>This package uses pre-commit and Tox for continuous integration (test, linting, etc.).</p>"},{"location":"contributing/#linting-and-type-checking-pre-commit","title":"Linting and Type Checking (pre-commit)","text":"<p>To use pre-commit, install the hook and then run against files.</p> <pre><code>$ pre-commit install\n$ pre-commit run --all-files\n</code></pre>"},{"location":"contributing/#tests-tox","title":"Tests (tox)","text":"<p>The entire CI workflow can be run with <code>$ tox</code>. This will test against multiple versions of Python and can be slow.</p> <p>Module-level unit-test are located in the <code>tests/</code> directory and its structure is intended to match that of <code>llm/</code>. E.g. the tests for <code>llm/preprocess/shard.py</code> are located in <code>tests/preprocess/shard.py</code>; however, additional test files can be added as needed. Tests should be narrowly focused and target a single aspect of the code's functionality, tests should not test internal implementation details of the code, and tests should not be dependent on the order in which they are run.</p> <p>Code that is useful for building tests but is not a test itself belongs in the <code>testing/</code> directory.</p> <pre><code># Run all tests in tests/\n$ tox -e py39\n# Run a specific test\n$ tox -e py39 -- tests/preprocess/shard.py::test_cli\n</code></pre>"},{"location":"contributing/#docs","title":"Docs","text":"<p>If code changes require an update to the documentation (e.g., for function signature changes, new modules, etc.), the documentation can be built using MKDocs.</p> <pre><code># With tox (will only build, does not serve)\n$ tox -e docs\n\n# Or manually\n$ pip install -e .[docs]\n$ mkdocs build --strict  # Build only to site/index.html\n$ mkdocs serve           # Serve locally\n</code></pre> <p>Docstrings are automatically generated, but it is recommended to check the generated docstrings to make sure details/links/etc. are correct.</p>"},{"location":"contributing/issues-pull-requests/","title":"Issues and Pull Requests","text":""},{"location":"contributing/issues-pull-requests/#issues","title":"Issues","text":"<p>Issue Tracker</p> <p>We use GitHub issues to report problems, request and track changes, and discuss future ideas. If you open an issue for a specific problem, please follow the template guides.</p>"},{"location":"contributing/issues-pull-requests/#pull-requests","title":"Pull Requests","text":"<p>We use the standard GitHub contribution cycle where all contributions are made via pull requests (including code owners!).</p> <ol> <li>Fork the repository and clone to your local machine.</li> <li>Create local changes.<ul> <li>Changes should conform to the style and testing guidelines, referenced   above.</li> <li>Preferred commit message format (source):<ul> <li>separate subject from body with a blank line,</li> <li>limit subject line to 50 characters,</li> <li>capitalize first word of subject line,</li> <li>do not end the subject line with a period,</li> <li>use the imperative mood for subject lines,</li> <li>include related issue numbers at end of subject line,</li> <li>wrap body at 72 characters, and</li> <li>use the body to explain what/why rather than how.   Example: <code>Fix concurrency bug in XYZ (#42)</code></li> </ul> </li> </ul> </li> <li>Push commits to your fork.<ul> <li>Please squash commits fixing mistakes to keep the git history clean.   For example, if commit \"b\" follows commit \"a\" and only fixes a small typo   from \"a\", please squash \"a\" and \"b\" into a single, correct commit.   This keeps the commit history readable and easier to search through when   debugging (e.g., git blame/bisect).</li> </ul> </li> <li>Open a pull request in this repository.<ul> <li>The pull request should include a description of the motivation for the   PR and included changes. A PR template is provided to guide this process.</li> </ul> </li> </ol>"},{"location":"contributing/style-guide/","title":"Style Guide","text":"<p>The Python code and docstring format mostly follows Google's Python Style Guide, but the pre-commit config is the authoritative source for code format compliance.</p> <p>Nits:</p> <ul> <li>Avoid imports in <code>__init__.py</code> (reduces the likelihood of circular imports).</li> <li>Prefer pure functions where possible.</li> <li>Define all class attributes inside <code>__init__</code> so all attributes are visible   in one place. Attributes that are defined later can be set as <code>None</code>   as a placeholder.</li> <li>Prefer f-strings (<code>f'name: {name}</code>) over string format   (<code>'name: {}'.format(name)</code>). Never use the <code>%</code> operator.</li> <li>Prefer typing.NamedTuple over collections.namedtuple.</li> <li>Exception messages should read as complete sentences with punctuation.   Logging messages can forgo trailing punctuation.   <pre><code>raise ValueError('Name must contain alphanumeric characters only.')\nlogger.info(f'New connection opened to {address}')\n</code></pre></li> <li>Document all exceptions that may be raised by a function in the docstring.</li> </ul>"},{"location":"guides/","title":"Guides","text":"<p>See the sidebar for the list of available guides.</p>"},{"location":"guides/bert-pretraining/","title":"BERT Pretraining","text":"<p>BERT pretraining based on NVIDIA's configuration.</p> <p>This guide assumes you have installed the <code>llm</code> packages and its dependencies as described in the Installation Guide.</p> <p>An example training configuration is provided in configs/bert-large/nvidia-lamb.py. Typically, you will need to change at least the <code>DATA_DIR</code>, <code>OUTPUT_DIR</code>, <code>RUN_NAME</code> and <code>PHASE</code> depending on your training state.</p>"},{"location":"guides/bert-pretraining/#run-commands","title":"Run Commands","text":"<ul> <li>Single-GPU for Debugging: <pre><code>python -m llm.trainers.bert --config configs/bert-large/nvidia-lamb.py --debug\n</code></pre></li> <li>Multi-GPU Single-Node: <pre><code>torchrun --nnodes=1 --nproc_per_node=auto --standalone \\\n-m llm.trainers.bert --config configs/bert-large/nvidia-lamb.py\n</code></pre></li> <li>Multi-Node Multi-GPU: <pre><code>torchrun --nnodes $NNODES --nproc_per_node=auto --max_restarts 0 \\\n--rdzv_backend=c10d --rdzv_endpoint=$PRIMARY_RANK \\\n-m llm.trainers.bert --config configs/bert-large/nvidia-lamb.py\n</code></pre></li> </ul>"},{"location":"guides/bert-pretraining/#pbs-training","title":"PBS Training","text":"<p>This is an example submission script for a PBS scheduler. <pre><code>#!/bin/bash\n#PBS -A __ALLOCATION__\n#PBS -q __QUEUE__\n#PBS -M __EMAIL__\n#PBS -m abe\n#PBS -l select=16:system=polaris\n#PBS -l walltime=6:00:00\n#PBS -l filesystems=home:grand\n#PBS -j oe\n# Figure out training environment based on PBS_NODEFILE existence\nif [[ -z \"${PBS_NODEFILE}\" ]]; then\nRANKS=$HOSTNAME\nNNODES=1\nelse\nPRIMARY_RANK=$(head -n 1 $PBS_NODEFILE)\nRANKS=$(tr '\\n' ' ' &lt; $PBS_NODEFILE)\nNNODES=$(&lt; $PBS_NODEFILE wc -l)\ncat $PBS_NODEFILE\nfi\nCONFIG=\"configs/bert-large/nvidia-lamb.py\"\n# Commands to run prior to the Python script for setting up the environment\nmodule load cray-python\nmodule load cudatoolkit-standalone/11.7.1\nsource /path/to/virtualenv\n\n# torchrun launch configuration\nLAUNCHER=\"torchrun \"\nLAUNCHER+=\"--nnodes=$NNODES --nproc_per_node=auto --max_restarts 0 \"\nif [[ \"$NNODES\" -eq 1 ]]; then\nLAUNCHER+=\"--standalone \"\nelse\nLAUNCHER+=\"--rdzv_backend=c10d --rdzv_endpoint=$PRIMARY_RANK\"\nfi\n# Training script and parameters\nCMD=\"$LAUNCHER -m llm.trainers.bert --config $CONFIG\"\necho \"Training Command: $CMD\"\nmpiexec --hostfile $PBS_NODEFILE -np $NNODES --env OMP_NUM_THREADS=8 --cpu-bind none $CMD\n</code></pre></p>"},{"location":"guides/bert-pretraining/#transitioning-from-phase-1-to-2","title":"Transitioning from Phase 1 to 2","text":"<p>After phase 1 training is complete, set <code>PHASE = 2</code> in the config file. Then, copy the last checkpoint from the <code>CHECKPOINT_DIR</code> for phase 1 to the new <code>CHECKPOINT_DIR</code> for phase 2 with the filename <code>global_step_0.pt</code>.</p>"},{"location":"guides/roberta-preprocessing/","title":"RoBERTa Pretraining Preprocessing Guide","text":"<p>This guide walks through downloading, formatting, and encoding the Wiki and Books corpora for pretraining RoBERTa.</p> <p>This guide assumes you have installed the <code>llm</code> packages and its dependencies as described in the Installation Guide.</p> <p>Note: using both the Wiki and Books corpora is not necessary, and you can skip one or the other. This instructions will also work for your own text corpora---just skip the download step.</p>"},{"location":"guides/roberta-preprocessing/#download-the-corpora","title":"Download the Corpora","text":"<p>Create a <code>datasets/</code> directory. This directory will contain all of the files produced. <pre><code>$ mkdir datasets/\n</code></pre></p> <p>Download the datasets. <pre><code>$ python -m llm.preprocess.download --dataset wikipedia --output datasets/downloaded/\n$ python -m llm.preprocess.download --dataset bookscorpus --output datasets/downloaded/\n</code></pre> This will result in two files: <code>datasets/downloaded/wikipedia-{date}.en.txt</code> and <code>datasets/downloaded/bookcorpus.txt</code>. Each of these files has the format one one sentence per line with documents separated by blank lines.</p>"},{"location":"guides/roberta-preprocessing/#shard-the-files","title":"Shard the Files","text":"<p>Next we shard the files to make them easier to work with. For small dataset this may not be needed.</p> <pre><code>$ python -m llm.preprocess.shard --input datasets/downloaded/*.txt --output datasets/sharded/wikibooks/ --size 250MB\n</code></pre> <p>Now we have a set of sharded files in <code>datasets/sharded/wikibooks/</code> that are each approximately 250 MB. The format of these files is still one sentence per line with documents separated by blank lines.</p>"},{"location":"guides/roberta-preprocessing/#build-the-vocab","title":"Build the Vocab","text":"<p>Now we build the vocab. BPE and wordpiece tokenizers are supported as well as cased/uncased. This example creates an uncased wordpiece vocab will 50,000 tokens.</p> <pre><code>$ python -m llm.preprocess.vocab \\\n      --input datasets/sharded/wikibooks \\\n      --output datasets/vocabs/wikibooks-50k-vocab.txt \\\n      --size 50000 \\\n      --tokenizers wordpiece \\\n</code></pre>"},{"location":"guides/roberta-preprocessing/#encode-the-shards","title":"Encode the Shards","text":"<p>Now we can encode the shards using our vocabulary.</p> <pre><code>$ python -m llm.preprocess.roberta \\\n      --input datasets/sharded/wikibooks/* \\\n      --output datasets/encoded/wikibooks/ \\\n      --vocab datasets/vocabs/wikibooks-50k-vocab.txt \\\n      --tokenizer wordpiece \\\n      --max-seq-len 512 \\\n      --processes 4\n</code></pre> <p>Encoding a 250MB shard can take around 16GB of RAM so adjust the number of processes (parallel workers encoding a shard) as needed to fit your shard size and system memory.</p> <p>This produces a set of encoded HDF5 files in <code>datasets/encoded/wikibooks</code> with each encoded file corresponding to a shard. Each encoded file contains the <code>input_ids</code>, <code>attention_masks</code>, and <code>special_tokens_masks</code> attributes which are each numpy arrays of shape <code>(samples, max_seq_len)</code>.</p> <p>In contrast to BERT encoding, the samples are not pre masked and next sentence prediction is not done. In other words, masking must be done at runtime and each sample represents contiguous sentences draw from the same document until the max sequence length is reached.</p>"},{"location":"installation/","title":"Installation","text":"<p>This package is Linux only and requires Python &gt;=3.9. It is recommended to install the package in a virtual environment of your choice. <pre><code>$ python -m venv venv     # or $ virtualenv venv\n$ . venv/bin/activate\n$ pip install torch       # torch install instructions may differ\n$ pip install .           # use -e for editable mode\n</code></pre> PyTorch installation instructions vary by system and CUDA versions so check the latest instructions here.</p> <p>ColossalAI can be installed to use the <code>FusedAdam</code> and <code>FusedLAMB</code> optimizers. See the directions here.</p>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<p>Development installation instructions are provided in the Contributing Guide.</p>"},{"location":"installation/#system-specific-installation","title":"System Specific Installation","text":"<p>Below are installation guides for specific HPC systems.</p> <ul> <li>Polaris</li> </ul>"},{"location":"installation/polaris/","title":"Polaris Installation","text":"<ol> <li>Load Polaris modules.    <pre><code>module load cray-python/3.9.12.1\nmodule load cudatoolkit-standalone/11.7.1\n</code></pre></li> <li>Clone and create your virtual environment.    <pre><code>git clone git@github.com:gpauloski/llm-pytorch\n# git clone https://github.com/gpauloski/llm-pytorch\ncd llm-pytorch\n</code></pre></li> <li>Create your virtual environment.    <pre><code>python -m venv venv\n. venv/bin/activate\n</code></pre> Note: anytime you want to use the virtual environment you will need to    load the above module and activate the virtual environment. It may be    helpful to add these to your <code>~/.bashrc</code> or PBS job scripts.</li> <li>Install PyTorch.    <pre><code>pip install torch==1.13.1\n</code></pre></li> <li>Install the <code>llm</code> package.    <pre><code>pip install -e .  # Use the [dev] extras install for developing\n</code></pre></li> <li>Install ColossalAI.    This can be done directly from pip (<code>pip install colossalai</code>) which will    JIT compile the CUDA extensions, or you can install with CUDA extensions    pre-build.    This must be done from a compute node.    E.g., <code>qsub -A [ALLOCATION] -l select=1:system=polaris -l walltime=1:00:00 -I -q debug -l filesystems=home:grand</code>.    <pre><code># Activate your modules and virtual environment\nmodule load cray-python/3.9.12.1 cudatoolkit-standalone/11.7.1\n. /path/to/venv/bin/activate\n\nmodule load gcc\n\nCUDA_EXT=1 pip install colossalai\n</code></pre></li> <li>Done!</li> </ol>"}]}